"""
æ–‡ä»¶æ‰«ææœåŠ¡

æ‰«ærawæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨åŒæ­¥åˆ°æ•°æ®åº“ã€‚
"""

import os
from pathlib import Path
from typing import Dict, List, Union

try:
    from astrbot.api import logger
except ImportError:
    import logging

    logger = logging.getLogger(__name__)
from ..llm_memory.components.file_index_manager import FileIndexManager
from ..llm_memory.service.note_service import NoteService


class FileMonitorService:
    """æ–‡ä»¶æ‰«ææœåŠ¡ç±»"""

    def __init__(
        self, data_directory: str, note_service: NoteService, config: dict = None
    ):
        """
        åˆå§‹åŒ–æ–‡ä»¶æ‰«ææœåŠ¡

        Args:
            data_directory: æ’ä»¶æ•°æ®ç›®å½•
            note_service: ç¬”è®°æœåŠ¡å®ä¾‹
            config: é…ç½®å­—å…¸
        """
        self.logger = logger
        self.data_directory = Path(data_directory)
        self.note_service = note_service

        # ä½¿ç”¨PathManagerç»Ÿä¸€ç®¡ç†è·¯å¾„
        path_manager = note_service.plugin_context.get_path_manager()
        self.raw_directory = path_manager.get_raw_dir()

        self.logger.info("æ–‡ä»¶ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆï¼ˆé¡ºåºå¤„ç†æ¨¡å¼ï¼‰")

        # åˆå§‹åŒ–FileIndexManagerç”¨äºå¢é‡åŒæ­¥
        provider_id = note_service.plugin_context.get_current_provider()
        self.file_index_manager = FileIndexManager(
            str(note_service.plugin_context.get_index_dir()), provider_id
        )

        # ç¡®ä¿rawç›®å½•å­˜åœ¨
        self.raw_directory.mkdir(parents=True, exist_ok=True)

        self.logger.info("æ–‡ä»¶æ‰«ææœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆå¢é‡åŒæ­¥æ¨¡å¼ï¼‰")
        self.logger.info(f"æ•°æ®ç›®å½•: {self.data_directory}")
        self.logger.info(f"æ‰«æç›®å½•: {self.raw_directory}")
        self.logger.info(f"æ‰«æç›®å½•å­˜åœ¨: {self.raw_directory.exists()}")

    def start_monitoring(self):
        """å¯åŠ¨æ–‡ä»¶æ‰«ææœåŠ¡ï¼ˆå¢é‡åŒæ­¥æ¨¡å¼ï¼‰"""
        try:
            self.logger.info("ğŸ”„ å¼€å§‹å¢é‡åŒæ­¥...")
            self._incremental_sync()
            self.logger.info("ğŸ“‚ æ–‡ä»¶æ‰«ææœåŠ¡å·²å®Œæˆ")

        except Exception as e:
            self.logger.error(f"å¯åŠ¨æ–‡ä»¶æ‰«ææœåŠ¡å¤±è´¥: {e}")
        finally:
            # å…³é”®ä¿®å¤ï¼šæ‰«æå®Œæˆåå½»åº•æ¸…ç†æ‰€æœ‰èµ„æº
            self._cleanup_all_resources()

    def stop_monitoring(self):
        """åœæ­¢æ–‡ä»¶æ‰«ææœåŠ¡"""
        try:
            self._cleanup_all_resources()
            self.logger.info("æ–‡ä»¶æ‰«ææœåŠ¡å·²åœæ­¢ã€‚")
        except Exception as e:
            self.logger.error(f"åœæ­¢æ–‡ä»¶æ‰«ææœåŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def _force_cleanup_connections(self):
        """å¼ºåˆ¶æ¸…ç†æ‰€æœ‰è¿æ¥ï¼ˆä»…åœ¨å¿…è¦æ—¶è°ƒç”¨ï¼‰"""
        try:
            # åªæœ‰åœ¨å†…å­˜å‹åŠ›å¤§æˆ–ç¨‹åºç»“æŸæ—¶æ‰å…³é—­æ‰€æœ‰è¿æ¥
            if hasattr(self.file_index_manager, "close"):
                self.file_index_manager.close()

            if hasattr(self.note_service, "id_service"):
                if hasattr(self.note_service.id_service, "close"):
                    self.note_service.id_service.close()
        except Exception:
            pass

    def _cleanup_all_resources(self):
        """å½»åº•æ¸…ç†æ‰€æœ‰èµ„æºï¼ˆæ‰«æå®Œæˆåè°ƒç”¨ï¼‰"""
        try:
            # 1. å¼ºåˆ¶å…³é—­SQLiteè¿æ¥ï¼ˆåªåœ¨ç¨‹åºç»“æŸæ—¶ï¼‰
            self._force_cleanup_connections()

            # 2. å…³é—­NoteServiceçš„çº¿ç¨‹æ± ï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(self.note_service, "_thread_pool"):
                self.note_service._thread_pool.shutdown(wait=True)
                self.logger.debug("âœ… NoteServiceçº¿ç¨‹æ± å·²å…³é—­")

            # 3. å¼ºåˆ¶ChromaDBæ‰§è¡ŒWAL checkpointï¼ˆæ¸…ç©ºWALæ–‡ä»¶ï¼‰
            self._force_chromadb_checkpoint("TRUNCATE")

            self.logger.info("ğŸ”“ æ‰€æœ‰èµ„æºå·²é‡Šæ”¾ï¼Œçº¿ç¨‹å·²å›æ”¶")

        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")

    def _force_chromadb_checkpoint(self, mode: str = "PASSIVE"):
        """
        ä¼˜åŒ–çš„ChromaDB WAL checkpointæœºåˆ¶

        Args:
            mode: checkpointæ¨¡å¼
                - PASSIVE: é»˜è®¤æ¨¡å¼ï¼Œä¸é˜»å¡å…¶ä»–æ“ä½œ
                - RESTART: æ›´å½»åº•çš„checkpoint
                - TRUNCATE: æ¸…ç©ºWALæ–‡ä»¶ï¼ˆæœ€å½»åº•ï¼‰
        """
        try:
            import sqlite3
            from pathlib import Path
            import time

            # è·å–ChromaDBæ•°æ®åº“è·¯å¾„
            vector_store = self.note_service.vector_store
            db_path = Path(vector_store.db_path) / "chroma.sqlite3"

            if not db_path.exists():
                self.logger.debug("ChromaDBæ•°æ®åº“ä¸å­˜åœ¨ï¼Œè·³è¿‡checkpoint")
                return

            # æ£€æŸ¥WALæ–‡ä»¶å¤§å°
            wal_path = db_path.with_suffix(".sqlite3-wal")
            wal_size = wal_path.stat().st_size if wal_path.exists() else 0

            # å¦‚æœWALæ–‡ä»¶å¤ªå°ï¼ˆ<1MBï¼‰ï¼Œè·³è¿‡checkpointï¼ˆå‡å°‘ä¸å¿…è¦å¼€é”€ï¼‰
            if wal_size < 1024 * 1024 and mode == "PASSIVE":
                self.logger.debug(f"WALæ–‡ä»¶è¾ƒå° ({wal_size} bytes)ï¼Œè·³è¿‡checkpoint")
                return

            start_time = time.time()

            # åˆ›å»ºä¸´æ—¶è¿æ¥æ‰§è¡Œcheckpoint
            conn = sqlite3.connect(str(db_path), timeout=30.0)
            try:
                # æ ¹æ®æ¨¡å¼é€‰æ‹©checkpointç­–ç•¥
                if mode == "TRUNCATE":
                    # æ¸…ç©ºWALæ–‡ä»¶ï¼ˆæœ€å½»åº•ï¼Œä½†å¯èƒ½é˜»å¡ï¼‰
                    conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                elif mode == "RESTART":
                    # é‡å¯WALæ¨¡å¼ï¼ˆä¸­ç­‰å¼ºåº¦ï¼‰
                    conn.execute("PRAGMA wal_checkpoint(RESTART)")
                else:
                    # é»˜è®¤PASSIVEæ¨¡å¼ï¼ˆæœ€è½»é‡ï¼‰
                    conn.execute("PRAGMA wal_checkpoint(PASSIVE)")

                conn.commit()

                # è®¡ç®—æ‰§è¡Œè€—æ—¶
                execution_time = time.time() - start_time

                # æ£€æŸ¥checkpointåçš„WALæ–‡ä»¶å¤§å°
                new_wal_size = wal_path.stat().st_size if wal_path.exists() else 0
                size_reduction = wal_size - new_wal_size

                self.logger.debug(
                    f"âœ… ChromaDB WAL checkpointå®Œæˆ | "
                    f"æ¨¡å¼: {mode} | "
                    f"è€—æ—¶: {execution_time:.2f}s | "
                    f"å‡å°‘: {size_reduction // 1024}KB"
                )

                # å¦‚æœWALæ–‡ä»¶ä»ç„¶è¿‡å¤§ä¸”ä¸æ˜¯TRUNCATEæ¨¡å¼ï¼Œè®°å½•è­¦å‘Š
                if new_wal_size > 10 * 1024 * 1024 and mode != "TRUNCATE":  # >10MB
                    self.logger.warning(
                        f"âš ï¸ WALæ–‡ä»¶ä»ç„¶è¾ƒå¤§ ({new_wal_size // 1024 // 1024}MB)ï¼Œ"
                        "å¯èƒ½éœ€è¦æ‰‹åŠ¨ç»´æŠ¤æˆ–ä½¿ç”¨TRUNCATEæ¨¡å¼"
                    )

            finally:
                conn.close()

        except Exception as e:
            # checkpointå¤±è´¥ä¸åº”è¯¥é˜»æ­¢å…¶ä»–æ¸…ç†æ“ä½œ
            self.logger.warning(f"ChromaDB checkpointå¤±è´¥ï¼ˆä¸å½±å“ç»§ç»­ï¼‰: {e}")

    def _format_timing_log(self, timings: dict) -> str:
        """æ ¼å¼åŒ–è®¡æ—¶ä¿¡æ¯ä¸ºæ—¥å¿—å­—ç¬¦ä¸²ï¼ˆæŒ‰å¤„ç†é¡ºåºï¼‰"""
        parts = []

        # 1. æ–‡ä»¶è§£æï¼ˆåˆ‡å— + IDæŸ¥è¯¢ï¼‰
        if "parse" in timings:
            parse_parts = [f"åˆ‡å—{timings['parse']:.0f}ms"]
            if "id_lookup" in timings and timings["id_lookup"] > 1:
                parse_parts.append(f"ID{timings['id_lookup']:.0f}ms")
            parts.append(f"æ–‡ä»¶è§£æï¼š{' + '.join(parse_parts)}")

        # 2. ä¸»é›†åˆï¼ˆå‘é‡åŒ– + DBï¼‰
        if "store_main" in timings:
            if "main_embed" in timings and "main_db" in timings:
                parts.append(
                    f"ä¸»é›†ï¼šå‘é‡{timings['main_embed']:.0f}ms + DB{timings['main_db']:.0f}ms"
                )
            else:
                parts.append(f"ä¸»é›†ï¼š{timings['store_main']:.0f}ms")

        # 3. å‰¯é›†åˆï¼ˆå‘é‡åŒ– + DBï¼‰
        if "store_sub" in timings:
            if "sub_embed" in timings and "sub_db" in timings:
                parts.append(
                    f"å‰¯é›†ï¼šå‘é‡{timings['sub_embed']:.0f}ms + DB{timings['sub_db']:.0f}ms"
                )
            else:
                parts.append(f"å‰¯é›†ï¼š{timings['store_sub']:.0f}ms")

        # 4. çº¿ç¨‹ç­‰å¾…ï¼ˆå¦‚æœæ˜¾è‘—ï¼‰
        if "_thread_wait" in timings and timings["_thread_wait"] > 100:
            parts.append(f"çº¿ç¨‹ç­‰å¾…ï¼š{timings['_thread_wait']:.0f}ms")

        return " | ".join(parts)

    # ===== å¢é‡åŒæ­¥åŠŸèƒ½ =====

    def _incremental_sync(self):
        """å¼‚æ­¥æ‰§è¡Œå¢é‡åŒæ­¥"""
        import time

        start_time = time.time()
        self.logger.info(f"å¼€å§‹å¢é‡åŒæ­¥: {self.raw_directory}")

        try:
            # 1. è·å–æ•°æ®åº“çŠ¶æ€
            old_files = self.file_index_manager.get_all_files()
            self.logger.debug(f"æ•°æ®åº“ä¸­æœ‰ {len(old_files)} ä¸ªæ–‡ä»¶è®°å½•")

            # 2. æ‰«ææ–‡ä»¶ç³»ç»Ÿ
            current_files = self._scan_directory_for_files(self.raw_directory)
            self.logger.debug(f"æ–‡ä»¶ç³»ç»Ÿä¸­æœ‰ {len(current_files)} ä¸ªæ–‡ä»¶")

            # 3. å¯¹æ¯”åˆ†æå˜æ›´
            changes = self._compare_file_states(old_files, current_files)
            self.logger.info(
                f"å˜æ›´æ£€æµ‹å®Œæˆ: åˆ é™¤ {len(changes['to_delete'])} ä¸ª, æ–°å¢/æ›´æ–° {len(changes['to_add'])} ä¸ª, æ— å˜åŒ– {len(changes['unchanged'])} ä¸ª"
            )

            # 4. æ‰§è¡Œåˆ é™¤æ“ä½œï¼ˆå…ˆåˆ é™¤æ—§æ•°æ®ï¼‰
            delete_count = 0
            if changes["to_delete"]:
                # æ”¶é›†æ‰€æœ‰éœ€è¦åˆ é™¤çš„æ–‡ä»¶ID
                file_ids = [file_id for file_id, _ in changes["to_delete"]]

                # æ‰¹é‡åˆ é™¤æ‰€æœ‰æ–‡ä»¶æ•°æ®
                if self._delete_file_data_by_file_id(file_ids):
                    delete_count = len(file_ids)
                    self.logger.info(f"æ‰¹é‡åˆ é™¤å®Œæˆ: {delete_count} ä¸ªæ–‡ä»¶")
                else:
                    self.logger.error("æ‰¹é‡åˆ é™¤æ–‡ä»¶æ•°æ®å¤±è´¥")

            # 5. æ‰§è¡Œæ–°å¢/æ›´æ–°æ“ä½œï¼ˆé¡ºåºå¤„ç†ï¼Œé¿å…ChromaDBé”ç«äº‰ï¼‰
            add_count = 0
            if changes["to_add"]:
                self.logger.info(
                    f"å¼€å§‹é¡ºåºå¤„ç† {len(changes['to_add'])} ä¸ªæ–°å¢/æ›´æ–°æ–‡ä»¶..."
                )

                # é¡ºåºå¤„ç†æ¯ä¸ªæ–‡ä»¶
                for idx, (relative_path, timestamp) in enumerate(changes["to_add"]):
                    try:
                        import time as time_module

                        file_start = time_module.time()

                        doc_count, timings = self._process_file_change(
                            relative_path, timestamp
                        )
                        if doc_count > 0:
                            add_count += 1

                        # è¯¦ç»†çš„å¤„ç†æ—¥å¿—
                        total_time = (time_module.time() - file_start) * 1000
                        from pathlib import Path

                        file_name = Path(relative_path).name
                        timing_str = self._format_timing_log(timings)

                        self.logger.info(
                            f"[{idx + 1}/{len(changes['to_add'])}] âœ… {file_name} | "
                            f"å—æ•°:{doc_count} | æ€»è€—æ—¶:{total_time:.0f}ms | {timing_str}"
                        )

                        # æ¯100ä¸ªæ–‡ä»¶æ˜¾ç¤ºè¿›åº¦
                        if (idx + 1) % 100 == 0:
                            progress = (idx + 1) / len(changes["to_add"]) * 100
                            self.logger.info(
                                f"ğŸ“Š è¿›åº¦: {progress:.1f}% ({idx + 1}/{len(changes['to_add'])})"
                            )

                    except Exception as e:
                        self.logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {relative_path}, é”™è¯¯: {e}")
                        continue

            # 6. è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = time.time() - start_time

            self.logger.info(
                f"å¢é‡åŒæ­¥å®Œæˆ: è€—æ—¶ {execution_time:.2f}s, åˆ é™¤ {delete_count} ä¸ªæ–‡ä»¶, æ–°å¢ {add_count} ä¸ªæ–‡ä»¶"
            )

        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.error(f"å¢é‡åŒæ­¥å¤±è´¥: {e}, è€—æ—¶ {execution_time:.2f}s")
            import traceback

            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

    def _scan_directory_for_files(self, directory_path: Path) -> Dict[str, int]:
        """
        æ‰«æç›®å½•ï¼Œè·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶åŠå…¶æ—¶é—´æˆ³ï¼ˆä¼˜åŒ–ç‰ˆï¼Œä½¿ç”¨os.walkï¼‰

        Args:
            directory_path: è¦æ‰«æçš„ç›®å½•è·¯å¾„

        Returns:
            å­—å…¸æ ¼å¼ï¼š{ç›¸å¯¹è·¯å¾„: æ—¶é—´æˆ³}
        """
        import time

        if not directory_path.exists() or not directory_path.is_dir():
            self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {directory_path}")
            return {}

        current_files = {}
        supported_extensions = {
            ".md",
            ".txt",
            ".pdf",
            ".docx",
            ".pptx",
            ".xlsx",
            ".html",
            ".csv",
            ".json",
            ".xml",
        }

        try:
            # ä½¿ç”¨os.walké€’å½’æ‰«æï¼ˆæ¯”Path.rglobå¿«å¾ˆå¤šï¼‰
            t_start = time.time()
            file_count = 0
            base_path = str(directory_path)
            base_path_len = len(base_path) + 1  # +1 for trailing slash

            for root, dirs, files in os.walk(base_path):
                for filename in files:
                    file_count += 1
                    # å¿«é€Ÿæ£€æŸ¥æ‰©å±•åï¼ˆé¿å…åˆ›å»ºPathå¯¹è±¡ï¼‰
                    _, ext = os.path.splitext(filename)
                    if ext.lower() in supported_extensions:
                        try:
                            # æ„å»ºå®Œæ•´è·¯å¾„
                            full_path = os.path.join(root, filename)

                            # è·å–æ–‡ä»¶æ—¶é—´æˆ³
                            timestamp = int(os.path.getmtime(full_path))

                            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼ˆå­—ç¬¦ä¸²åˆ‡ç‰‡ï¼Œæ¯”Path.relative_toå¿«ï¼‰
                            if full_path.startswith(base_path):
                                relative_path = full_path[base_path_len:].replace(
                                    "\\", "/"
                                )
                            else:
                                relative_path = os.path.relpath(
                                    full_path, base_path
                                ).replace("\\", "/")

                            current_files[relative_path] = timestamp
                        except (OSError, ValueError) as e:
                            self.logger.warning(
                                f"æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯: {full_path}, é”™è¯¯: {e}"
                            )
                            continue

            scan_time = time.time() - t_start
            self.logger.info(
                f"âœ… æ‰«æå®Œæˆï¼Œå‘ç° {len(current_files)} ä¸ªæ”¯æŒçš„æ–‡ä»¶ï¼ˆå…±{file_count}ä¸ªæ–‡ä»¶ï¼‰ | è€—æ—¶: {scan_time:.2f}ç§’"
            )
            return current_files

        except Exception as e:
            self.logger.error(f"æ‰«æç›®å½•å¤±è´¥: {directory_path}, é”™è¯¯: {e}")
            return {}

    def _compare_file_states(
        self, old_files: List[Dict], current_files: Dict[str, int]
    ) -> Dict:
        """
        æ¯”è¾ƒæ–‡ä»¶çŠ¶æ€ï¼Œè¯†åˆ«å˜æ›´

        Args:
            old_files: æ•°æ®åº“ä¸­çš„æ–‡ä»¶åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{id: int, relative_path: str, file_timestamp: int}]
            current_files: å½“å‰æ–‡ä»¶ç³»ç»ŸçŠ¶æ€ï¼Œæ ¼å¼ï¼š{ç›¸å¯¹è·¯å¾„: æ—¶é—´æˆ³}

        Returns:
            å˜æ›´åˆ†æç»“æœï¼Œæ ¼å¼ï¼š
            {
                "to_delete": [(file_id, relative_path)],  # å·²åˆ é™¤æˆ–éœ€è¦é‡æ–°å¤„ç†çš„æ–‡ä»¶
                "to_add": [(relative_path, timestamp)],    # æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶
                "unchanged": [(file_id, relative_path)]    # æ— å˜åŒ–çš„æ–‡ä»¶
            }
        """
        # æ„å»ºæ•°æ®åº“æ–‡ä»¶çš„å¿«é€ŸæŸ¥æ‰¾å­—å…¸
        db_files = {}
        for file_info in old_files:
            db_files[file_info["relative_path"]] = file_info

        to_delete = []
        to_add = []
        unchanged = []

        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ–‡ä»¶ï¼ˆæŸ¥æ‰¾å·²åˆ é™¤æˆ–æ—¶é—´æˆ³å˜åŒ–çš„æ–‡ä»¶ï¼‰
        for relative_path, file_info in db_files.items():
            if relative_path not in current_files:
                # æ–‡ä»¶å·²åˆ é™¤
                to_delete.append((file_info["id"], relative_path))
            elif current_files[relative_path] > file_info["file_timestamp"]:
                # æ–‡ä»¶æ—¶é—´æˆ³æ›´æ–°ï¼Œéœ€è¦é‡æ–°å¤„ç†
                to_delete.append((file_info["id"], relative_path))
                to_add.append((relative_path, current_files[relative_path]))
            else:
                # æ–‡ä»¶æ— å˜åŒ–
                unchanged.append((file_info["id"], relative_path))

        # æ£€æŸ¥å½“å‰æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–°æ–‡ä»¶
        for relative_path, timestamp in current_files.items():
            if relative_path not in db_files:
                # æ–°æ–‡ä»¶
                to_add.append((relative_path, timestamp))

        return {"to_delete": to_delete, "to_add": to_add, "unchanged": unchanged}

    def _process_file_change(self, relative_path: str, timestamp: int) -> tuple:
        """å¤„ç†å•ä¸ªæ–‡ä»¶çš„å˜æ›´ï¼Œè¿”å›(æ–‡æ¡£æ•°é‡, è®¡æ—¶å­—å…¸)ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
            full_path = self.raw_directory / relative_path

            if not full_path.exists():
                self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
                return 0, {}

            # å°å¼Ÿå‘é¢†å¯¼ç”³è¯·file_idï¼ˆé¢†å¯¼ä¸²è¡Œåˆ†é…ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ›å»º5800ä¸ªï¼‰
            file_id = self.file_index_manager.get_or_create_file_id(
                relative_path, timestamp
            )

            try:
                # å°å¼Ÿå¤„ç†æ–‡ä»¶ï¼Œä½¿ç”¨é¢†å¯¼åˆ†é…çš„file_idï¼ˆåŒæ­¥è°ƒç”¨ï¼‰
                doc_count, timings = self.note_service.parse_and_store_file_sync(
                    str(full_path), relative_path
                )
                return doc_count, timings
            except Exception as e:
                # å¤±è´¥äº†ï¼Œå›æ»šè¿™ä¸ªfile_id
                self.logger.error(
                    f"æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œå›æ»šfile_id: {relative_path}, é”™è¯¯: {e}"
                )
                try:
                    # ä½¿ç”¨æ”¹é€ åçš„æ–¹æ³•ï¼Œæ”¯æŒå•ä¸ªæ–‡ä»¶åˆ é™¤
                    self._delete_file_data_by_file_id(file_id)
                    self.logger.debug(
                        f"å·²å›æ»šæ–‡ä»¶ç´¢å¼•: {relative_path} (ID: {file_id})"
                    )
                except Exception as rollback_error:
                    self.logger.error(
                        f"å›æ»šæ–‡ä»¶ç´¢å¼•å¤±è´¥: {relative_path}, é”™è¯¯: {rollback_error}"
                    )
                raise

        except Exception as e:
            self.logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {relative_path}, é”™è¯¯: {e}")
            return 0, {}

    def _delete_file_data_by_file_id(self, file_ids: Union[int, List[int]]) -> bool:
        """
        åˆ é™¤æ–‡ä»¶ç›¸å…³çš„æ‰€æœ‰æ•°æ®ï¼ˆæ”¯æŒå•ä¸ªå’Œæ‰¹é‡åˆ é™¤ï¼‰

        Args:
            file_ids: å•ä¸ªæ–‡ä»¶IDæˆ–æ–‡ä»¶IDåˆ—è¡¨

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        # ç»Ÿä¸€å¤„ç†è¾“å…¥å‚æ•°
        if isinstance(file_ids, int):
            file_ids = [file_ids]
        elif not file_ids:
            return True

        try:
            self.logger.info(f"å¼€å§‹åˆ é™¤ {len(file_ids)} ä¸ªæ–‡ä»¶çš„æ•°æ®")

            # 1. æ‰¹é‡æŸ¥è¯¢ä¸»é›†åˆï¼Œè·å–æ‰€æœ‰éœ€è¦åˆ é™¤çš„ç¬”è®°ID
            where_clause = {"file_id": {"$in": file_ids}}
            main_results = self.note_service.main_collection.get(where=where_clause)
            ids_to_delete = (
                main_results["ids"] if main_results and main_results["ids"] else []
            )

            self.logger.debug(f"éœ€è¦åˆ é™¤ {len(ids_to_delete)} ä¸ªç¬”è®°æ–‡æ¡£")

            # 2. æ‰¹é‡åˆ é™¤å‰¯é›†åˆï¼ˆåŸºäºç¬”è®°IDï¼‰
            if ids_to_delete:
                self.note_service.sub_collection.delete(ids=ids_to_delete)
                self.logger.debug(f"å·²åˆ é™¤å‰¯é›†åˆçš„ {len(ids_to_delete)} ä¸ªæ–‡æ¡£")

            # 3. æ‰¹é‡åˆ é™¤ä¸»é›†åˆï¼ˆåŸºäºæ–‡ä»¶IDï¼‰
            self.note_service.main_collection.delete(where=where_clause)
            self.logger.debug(f"å·²ä»ä¸»é›†åˆä¸­åˆ é™¤ä¸ {len(file_ids)} ä¸ªæ–‡ä»¶ç›¸å…³çš„æ–‡æ¡£")

            # 4. æ‰¹é‡åˆ é™¤SQLiteè®°å½•
            self._batch_delete_sqlite_records(file_ids)

            return True
        except Exception as e:
            self.logger.error(f"åˆ é™¤æ–‡ä»¶æ•°æ®å¤±è´¥ (IDs: {file_ids}): {e}")
            return False

    def _batch_delete_sqlite_records(self, file_ids: List[int]) -> bool:
        """æ‰¹é‡åˆ é™¤SQLiteè®°å½•å’Œå†…å­˜ç¼“å­˜"""
        try:
            table_name = self.file_index_manager._get_table_name()
            placeholders = ",".join(["?" for _ in file_ids])

            # æ‰¹é‡æŸ¥è¯¢æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºç¼“å­˜æ¸…ç†ï¼‰
            select_query = f"SELECT id, relative_path FROM {table_name} WHERE id IN ({placeholders})"
            cursor = self.file_index_manager._execute_query(
                select_query, tuple(file_ids)
            )
            file_mappings = cursor.fetchall()  # [(id, path), (id, path), ...]

            # æ‰¹é‡åˆ é™¤SQLiteè®°å½•ï¼ˆä½¿ç”¨æ–°çš„å¯é æ–¹æ³•ï¼‰
            delete_query = f"DELETE FROM {table_name} WHERE id = ?"
            params_list = [(file_id,) for file_id in file_ids]
            deleted_count = self.file_index_manager._execute_batch_delete(
                delete_query, params_list, caller="batch_delete_sqlite"
            )
            self.logger.debug(
                f"è¯·æ±‚åˆ é™¤ {len(file_ids)} ä¸ªSQLiteè®°å½•ï¼Œé€šè¿‡æ–°çš„æ‰¹é‡æ–¹æ³•å®é™…åˆ é™¤äº† {deleted_count} ä¸ªã€‚"
            )

            # æ‰¹é‡æ¸…ç†å†…å­˜ç¼“å­˜
            with self.file_index_manager._cache_lock:
                for file_id, relative_path in file_mappings:
                    self.file_index_manager._id_cache.pop(file_id, None)
                    self.file_index_manager._path_cache.pop(relative_path, None)

            self.logger.debug(f"å·²æ¸…ç† {len(file_mappings)} ä¸ªæ–‡ä»¶çš„å†…å­˜ç¼“å­˜")
            return True
        except Exception as e:
            self.logger.error(f"åˆ é™¤SQLiteè®°å½•å¤±è´¥: {e}")
            return False
