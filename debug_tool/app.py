import datetime
import json
import math
import os

import streamlit as st

from utils.config_loader import ConfigLoader
from utils.db import DBManager

st.set_page_config(
    page_title="Angel Memory Debug Tool",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown(
    """
<style>
    .stTextArea textarea { font-family: Consolas, Monaco, monospace; }
    .main .block-container { padding-top: 1.2rem; }
</style>
""",
    unsafe_allow_html=True,
)


@st.cache_resource
def get_managers():
    loader = ConfigLoader()
    provider = loader.get_embedding_provider()
    db_mgr = DBManager(loader, provider)
    return loader, db_mgr, loader.get_raw_notes_dir()


def render_item(item, show_hit_count: bool = False):
    meta = item.get("metadata", {}) or {}
    content = item.get("document") or meta.get("judgment") or "*[æ— å†…å®¹]*"
    header = []
    if meta.get("memory_type"):
        header.append(f"ğŸ·ï¸ `{meta['memory_type']}`")
    if show_hit_count and item.get("hit_count") is not None:
        header.append(f"ğŸ¯ å‘½ä¸­æ ‡ç­¾æ•° `{item.get('hit_count')}`")
    if meta.get("memory_scope"):
        header.append(f"ğŸ”’ scope `{meta.get('memory_scope')}`")
    if meta.get("created_at"):
        try:
            ts = float(meta.get("created_at"))
            if ts > 1e11:
                ts /= 1000
            header.append(
                f"ğŸ•’ {datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')}"
            )
        except Exception:
            pass
    if meta.get("strength") is not None:
        header.append(f"ğŸ’ª `{meta.get('strength')}`")
    if meta.get("tags"):
        header.append(f"ğŸ”– {meta.get('tags')}")
    if header:
        st.markdown(" | ".join(header))
    st.markdown(content)


loader, db_mgr, raw_dir = get_managers()

with st.sidebar:
    st.title("ğŸ§  Angel Memory")
    mode = st.radio(
        "è°ƒè¯•æ¨¡å¼",
        [
            "ğŸ“Œ æ€»è§ˆ",
            "ğŸ§¾ ä¸­å¤®è®°å¿†æµè§ˆ",
            "ğŸ”– å…¨å±€Tagsè°ƒè¯•",
            "ğŸ§­ memory_index æ£€ç´¢",
            "ğŸ”„ ä¸­å¤®åº“å¯¼å…¥å¯¼å‡º",
            "ğŸ› ï¸ ç»´æŠ¤çŠ¶æ€",
            "ğŸ“‚ æµè§ˆç¬”è®°æ–‡ä»¶",
        ],
        index=0,
    )
    st.divider()
    overview = db_mgr.get_overview()
    st.caption(f"Provider: {overview.get('provider_id')}")
    st.caption(f"ä¸­å¤®è®°å¿†: {overview.get('memory_count', 0)}")
    st.caption(f"å…¨å±€Tags: {overview.get('global_tag_count', 0)}")
    st.caption(f"memory_index: {overview.get('memory_index_count', 0)}")

if mode == "ğŸ“Œ æ€»è§ˆ":
    st.subheader("ğŸ“Œ å½“å‰è°ƒè¯•ç¯å¢ƒ")
    ov = db_mgr.get_overview()
    st.json(ov)

    c1, c2 = st.columns(2)
    with c1:
        st.markdown("**è·¯å¾„çŠ¶æ€**")
        st.write(f"- central db: `{ov.get('simple_db_path')}`")
        st.write(f"- maintenance: `{ov.get('maintenance_state_path')}`")
        st.write(f"- backups: `{ov.get('backup_dir')}`")
        st.write(f"- chromadb: `{ov.get('chromadb_path')}`")
    with c2:
        st.markdown("**scope åˆ—è¡¨**")
        scopes = ov.get("scopes") or []
        if scopes:
            for s in scopes:
                st.write(f"- `{s}`")
        else:
            st.caption("æš‚æ— ")

elif mode == "ğŸ§¾ ä¸­å¤®è®°å¿†æµè§ˆ":
    st.subheader("ğŸ§¾ ä¸­å¤®è®°å¿†æµè§ˆï¼ˆmemory_center/index/simple_memory.dbï¼‰")
    if not db_mgr.has_central_db():
        st.error("æœªæ‰¾åˆ°ä¸­å¤®è®°å¿†åº“ã€‚")
        st.stop()

    stats = db_mgr.get_central_stats()
    scopes = stats.get("scopes", [])
    c1, c2 = st.columns([2, 3])
    with c1:
        selected_scope = st.selectbox("scope è¿‡æ»¤", ["(å…¨éƒ¨)"] + scopes, index=0)
    with c2:
        keyword = st.text_input("å…³é”®è¯ï¼ˆjudgment/reasoning/tagsï¼‰", value="")

    page_size = 20
    current_page = int(st.session_state.get("central_page", 1) or 1)
    scope_filter = "" if selected_scope == "(å…¨éƒ¨)" else selected_scope
    offset = max(0, (current_page - 1) * page_size)
    items, total = db_mgr.browse_central_memories(
        limit=page_size,
        offset=offset,
        scope=scope_filter,
        keyword=keyword,
        return_total=True,
    )
    total_pages = math.ceil(total / page_size) if total > 0 else 1

    col_p1, col_p2 = st.columns([1, 3])
    with col_p1:
        page = st.number_input(
            f"é¡µç  (å…± {total_pages} é¡µ)",
            min_value=1,
            max_value=max(1, total_pages),
            value=min(current_page, total_pages),
            key="central_page",
        )
    if page != current_page:
        offset = max(0, (page - 1) * page_size)
        items, total = db_mgr.browse_central_memories(
            limit=page_size,
            offset=offset,
            scope=scope_filter,
            keyword=keyword,
            return_total=True,
        )
    st.caption(f"æ€»è®°å½• {total}ï¼Œå½“å‰é¡µ {len(items)} æ¡")
    for item in items:
        with st.container(border=True):
            render_item(item)
            with st.expander("å…ƒæ•°æ®"):
                st.json(item.get("metadata", {}))

elif mode == "ğŸ”– å…¨å±€Tagsè°ƒè¯•":
    st.subheader("ğŸ”– å…¨å±€ Tags è°ƒè¯•ï¼ˆglobal_tags + memory_tag_rel + note_tag_relï¼‰")
    if not db_mgr.has_central_db():
        st.error("æœªæ‰¾åˆ°ä¸­å¤®è®°å¿†åº“ã€‚")
        st.stop()

    c1, c2, c3 = st.columns([3, 2, 2])
    with c1:
        query_text = st.text_input("è¾“å…¥åŸå§‹é—®é¢˜ï¼ˆç”¨äº tags å‘½ä¸­ï¼‰", value="")
    with c2:
        scope_for_hit = st.text_input("scopeï¼ˆå¯ç©ºï¼‰", value="")
    with c3:
        hit_limit = st.number_input("å‘½ä¸­è®°å¿†ä¸Šé™", min_value=1, max_value=200, value=30)

    if query_text:
        result = db_mgr.unified_tag_hit_search(query_text, limit=int(hit_limit), scope=scope_for_hit)
        st.markdown("**å‘½ä¸­ tags**")
        st.write(result.get("matched_tags", []))
        st.caption(f"matched_tag_ids: {result.get('matched_tag_ids', [])}")

        st.markdown("**å‘½ä¸­è®°å¿†**")
        hits = result.get("memory_hits", [])
        if hits:
            for item in hits:
                with st.container(border=True):
                    render_item(item, show_hit_count=True)
                    with st.expander("å…ƒæ•°æ®"):
                        st.json(item.get("metadata", {}))
        else:
            st.info("æ— å‘½ä¸­è®°å¿†ã€‚")

    st.divider()
    st.markdown("**å…¨å±€ tags åˆ—è¡¨**")
    tag_keyword = st.text_input("ç­›é€‰ tag name", value="", key="tag_keyword_filter")
    tags = db_mgr.get_global_tags(limit=300, offset=0, keyword=tag_keyword)
    st.caption(f"å…±è¿”å› {len(tags)} æ¡")
    if tags:
        st.dataframe(tags, use_container_width=True, hide_index=True)

elif mode == "ğŸ§­ memory_index æ£€ç´¢":
    st.subheader("ğŸ§­ å‘é‡è½»é‡ç´¢å¼•è°ƒè¯•ï¼ˆcollection = memory_indexï¼‰")
    if not db_mgr.has_vector_db():
        st.error("æœªè¿æ¥å‘é‡åº“ã€‚")
        st.stop()

    collections = db_mgr.get_collections()
    if "memory_index" not in collections:
        st.warning("æœªæ‰¾åˆ° `memory_index` é›†åˆã€‚")
    else:
        st.caption(f"memory_index count: {db_mgr.get_collection_stats('memory_index').get('count', 0)}")
        query = st.text_input("å‘é‡æŸ¥è¯¢", value="", placeholder="è¾“å…¥ä¸€å¥è¯æµ‹è¯•å‘é‡å¬å›")
        topk = st.slider("Top K", min_value=1, max_value=30, value=10)
        if query:
            results = db_mgr.query_collection(
                collection_name="memory_index",
                query_text=query,
                n_results=int(topk),
            )
            for item in results:
                if item.get("error"):
                    st.error(item.get("error"))
                    continue
                with st.container(border=True):
                    st.markdown(f"**score:** `{item.get('score', 0):.4f}` | **id:** `{item.get('id')}`")
                    st.code(item.get("document") or "", language="text")
                    if item.get("metadata"):
                        st.caption(f"metadata: {item.get('metadata')}")

        st.divider()
        st.markdown("**åŸå§‹æµè§ˆ**")
        page_size = 20
        page = st.number_input("é¡µç ", min_value=1, value=1, key="memory_index_page")
        offset = (int(page) - 1) * page_size
        items = db_mgr.browse_collection("memory_index", limit=page_size, offset=offset)
        for item in items:
            with st.container(border=True):
                st.markdown(f"**id:** `{item.get('id')}`")
                st.code(item.get("document") or "", language="text")
                if item.get("metadata"):
                    st.caption(f"metadata: {item.get('metadata')}")

elif mode == "ğŸ”„ ä¸­å¤®åº“å¯¼å…¥å¯¼å‡º":
    st.subheader("ğŸ”„ ä¸­å¤®è®°å¿†åº“ JSON å¯¼å…¥å¯¼å‡º")
    if not db_mgr.has_central_db():
        st.error("æœªæ‰¾åˆ°ä¸­å¤®è®°å¿†åº“ã€‚")
        st.stop()

    c1, c2 = st.columns(2)
    with c1:
        st.markdown("**å¯¼å‡ºä¸­å¤®å¿«ç…§**")
        if st.button("ç”Ÿæˆå¿«ç…§", key="btn_export_central"):
            st.session_state["central_snapshot"] = db_mgr.export_central_snapshot()
            st.success("å¿«ç…§å·²ç”Ÿæˆã€‚")
        snapshot = st.session_state.get("central_snapshot")
        if snapshot:
            t = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                "ä¸‹è½½ JSON",
                data=json.dumps(snapshot, ensure_ascii=False, indent=2),
                file_name=f"memory_center_snapshot_{t}.json",
                mime="application/json",
            )

    with c2:
        st.markdown("**å¯¼å…¥ JSON**")
        uploaded = st.file_uploader("é€‰æ‹© JSON æ–‡ä»¶", type=["json"])
        if uploaded is not None and st.button("æ‰§è¡Œå¯¼å…¥", key="btn_import_central"):
            try:
                payload = json.loads(uploaded.read().decode("utf-8"))
                stats = db_mgr.import_central_payload(payload)
                st.success(
                    f"å¯¼å…¥å®Œæˆï¼šæ–°å¢ {stats.get('inserted', 0)} / æ›´æ–° {stats.get('upserted', 0)} / "
                    f"è·³è¿‡ {stats.get('skipped', 0)} / å¤±è´¥ {stats.get('failed', 0)}"
                )
            except Exception as e:
                st.error(f"å¯¼å…¥å¤±è´¥ï¼š{e}")

elif mode == "ğŸ› ï¸ ç»´æŠ¤çŠ¶æ€":
    st.subheader("ğŸ› ï¸ ç»´æŠ¤çŠ¶æ€ä¸å¤‡ä»½")
    state = db_mgr.get_maintenance_state()
    if state:
        st.markdown("**maintenance_state.json**")
        st.json(state)
    else:
        st.warning("æœªæ‰¾åˆ° maintenance_state.json æˆ–æ–‡ä»¶ä¸ºç©ºã€‚")

    st.divider()
    st.markdown("**å¤‡ä»½æ–‡ä»¶ï¼ˆæœ€å¤šä¿ç•™3ä¸ªï¼‰**")
    backups = db_mgr.list_backups()
    if backups:
        st.dataframe(backups, use_container_width=True, hide_index=True)
        selected = st.selectbox(
            "é€‰æ‹©å¤‡ä»½é¢„è§ˆ",
            [b["name"] for b in backups],
            index=0,
        )
        selected_path = next((b["path"] for b in backups if b["name"] == selected), "")
        if selected_path:
            st.json(db_mgr.load_backup_preview(selected_path))
    else:
        st.info("æš‚æ— å¤‡ä»½æ–‡ä»¶ã€‚")

elif mode == "ğŸ“‚ æµè§ˆç¬”è®°æ–‡ä»¶":
    st.subheader("ğŸ“‚ raw ç›®å½•ç¬”è®°é¢„è§ˆï¼ˆä»…æ–‡ä»¶è§†å›¾ï¼‰")
    if not os.path.exists(raw_dir):
        st.error(f"ç¬”è®°ç›®å½•ä¸å­˜åœ¨: {raw_dir}")
        st.stop()

    md_files = []
    for root, _, files in os.walk(raw_dir):
        for f in files:
            if f.endswith(".md"):
                abs_path = os.path.join(root, f)
                rel_path = os.path.relpath(abs_path, raw_dir).replace("\\", "/")
                md_files.append(rel_path)
    md_files.sort()

    if not md_files:
        st.info("æœªå‘ç° markdown æ–‡ä»¶ã€‚")
        st.stop()

    selected = st.selectbox("é€‰æ‹©æ–‡ä»¶", md_files, index=0)
    full_path = os.path.join(raw_dir, selected.replace("/", os.sep))
    try:
        with open(full_path, "r", encoding="utf-8") as f:
            content = f.read()
        st.caption(selected)
        st.markdown(content)
    except Exception as e:
        st.error(f"è¯»å–å¤±è´¥: {e}")

