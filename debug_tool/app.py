import datetime
import json
import math
import os
import re

import streamlit as st

from utils.config_loader import ConfigLoader
from utils.db import DBManager

st.set_page_config(
    page_title="Angel Memory Debug Tool",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown(
    """
<style>
    .stTextArea textarea { font-family: Consolas, Monaco, monospace; }
    .main .block-container { padding-top: 1.2rem; }
</style>
""",
    unsafe_allow_html=True,
)


@st.cache_resource
def get_managers():
    loader = ConfigLoader()
    provider = loader.get_embedding_provider()
    db_mgr = DBManager(loader, provider)
    return loader, db_mgr, loader.get_raw_notes_dir()


def render_item(item, show_hit_count: bool = False):
    meta = item.get("metadata", {}) or {}
    content = item.get("document") or meta.get("judgment") or "*[æ— å†…å®¹]*"
    header = []
    if meta.get("memory_type"):
        header.append(f"ğŸ·ï¸ `{meta['memory_type']}`")
    if show_hit_count and item.get("hit_count") is not None:
        header.append(f"ğŸ¯ å‘½ä¸­æ ‡ç­¾æ•° `{item.get('hit_count')}`")
    if meta.get("memory_scope"):
        header.append(f"ğŸ”’ scope `{meta.get('memory_scope')}`")
    if meta.get("created_at"):
        try:
            ts = float(meta.get("created_at"))
            if ts > 1e11:
                ts /= 1000
            header.append(
                f"ğŸ•’ {datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')}"
            )
        except Exception:
            pass
    if meta.get("strength") is not None:
        header.append(f"ğŸ’ª `{meta.get('strength')}`")
    if meta.get("tags"):
        header.append(f"ğŸ”– {meta.get('tags')}")
    if header:
        st.markdown(" | ".join(header))
    st.markdown(content)


def _normalize_text(value: str) -> str:
    return re.sub(r"\s+", " ", str(value or "").strip())


def _scan_md_headings(lines):
    heading_re = re.compile(r"^(#{1,6})\s+(.+)$")
    rows = []
    for idx, line in enumerate(lines):
        m = heading_re.match(line)
        if not m:
            continue
        rows.append((len(m.group(1)), _normalize_text(m.group(2)), idx))
    return rows


def _extract_markdown_segment(text: str, h_values, heading_level: int, heading_text: str):
    lines = text.split("\n")
    headings = _scan_md_headings(lines)
    if not headings:
        return text, []

    available = [(lvl, title) for lvl, title, _ in headings]
    target_start = -1
    target_level = 0
    if heading_level and heading_text:
        target = _normalize_text(heading_text)
        for level, title, line_no in headings:
            if level == heading_level and _normalize_text(title) == target:
                target_start = line_no
                target_level = level
                break
    else:
        expected = [_normalize_text(v) for v in h_values]
        current = ["", "", "", "", "", ""]
        for level, title, line_no in headings:
            current[level - 1] = _normalize_text(title)
            for i in range(level, 6):
                current[i] = ""
            matched = True
            for i, val in enumerate(expected):
                if val and current[i] != val:
                    matched = False
                    break
            if matched and any(expected):
                target_start = line_no
                target_level = max([i + 1 for i, v in enumerate(expected) if v], default=0)
                break

    if target_start < 0:
        return "", available

    target_end = len(lines)
    for level, _, line_no in headings:
        if line_no <= target_start:
            continue
        if level <= target_level:
            target_end = line_no
            break
    return "\n".join(lines[target_start:target_end]).strip(), available


loader, db_mgr, raw_dir = get_managers()

with st.sidebar:
    st.title("ğŸ§  Angel Memory")
    mode = st.radio(
        "è°ƒè¯•æ¨¡å¼",
        [
            "ğŸ“Œ æ€»è§ˆ",
            "ğŸ§¾ ä¸­å¤®è®°å¿†æµè§ˆ",
            "ğŸ”– å…¨å±€Tagsè°ƒè¯•",
            "ğŸ§­ memory_index æ£€ç´¢",
            "ğŸ—‚ï¸ ä¸­å¤®ç¬”è®°ç´¢å¼•",
            "ğŸ§  notes_index æ£€ç´¢",
            "ğŸ“ note_recall æ¨¡æ‹Ÿ",
            "ğŸ”„ ä¸­å¤®åº“å¯¼å…¥å¯¼å‡º",
            "ğŸ› ï¸ ç»´æŠ¤çŠ¶æ€",
            "ğŸ“‚ æµè§ˆç¬”è®°æ–‡ä»¶",
        ],
        index=0,
    )
    st.divider()
    overview = db_mgr.get_overview()
    st.caption(f"Provider: {overview.get('provider_id')}")
    st.caption(f"ä¸­å¤®è®°å¿†: {overview.get('memory_count', 0)}")
    st.caption(f"å…¨å±€Tags: {overview.get('global_tag_count', 0)}")
    st.caption(f"memory_index: {overview.get('memory_index_count', 0)}")

if mode == "ğŸ“Œ æ€»è§ˆ":
    st.subheader("ğŸ“Œ å½“å‰è°ƒè¯•ç¯å¢ƒ")
    ov = db_mgr.get_overview()
    st.json(ov)

    c1, c2 = st.columns(2)
    with c1:
        st.markdown("**è·¯å¾„çŠ¶æ€**")
        st.write(f"- central db: `{ov.get('simple_db_path')}`")
        st.write(f"- maintenance: `{ov.get('maintenance_state_path')}`")
        st.write(f"- backups: `{ov.get('backup_dir')}`")
        st.write(f"- chromadb: `{ov.get('chromadb_path')}`")
    with c2:
        st.markdown("**scope åˆ—è¡¨**")
        scopes = ov.get("scopes") or []
        if scopes:
            for s in scopes:
                st.write(f"- `{s}`")
        else:
            st.caption("æš‚æ— ")

elif mode == "ğŸ§¾ ä¸­å¤®è®°å¿†æµè§ˆ":
    st.subheader("ğŸ§¾ ä¸­å¤®è®°å¿†æµè§ˆï¼ˆmemory_center/index/simple_memory.dbï¼‰")
    if not db_mgr.has_central_db():
        st.error("æœªæ‰¾åˆ°ä¸­å¤®è®°å¿†åº“ã€‚")
        st.stop()

    stats = db_mgr.get_central_stats()
    scopes = stats.get("scopes", [])
    c1, c2 = st.columns([2, 3])
    with c1:
        selected_scope = st.selectbox("scope è¿‡æ»¤", ["(å…¨éƒ¨)"] + scopes, index=0)
    with c2:
        keyword = st.text_input("å…³é”®è¯ï¼ˆjudgment/reasoning/tagsï¼‰", value="")

    page_size = 20
    current_page = int(st.session_state.get("central_page", 1) or 1)
    scope_filter = "" if selected_scope == "(å…¨éƒ¨)" else selected_scope
    offset = max(0, (current_page - 1) * page_size)
    items, total = db_mgr.browse_central_memories(
        limit=page_size,
        offset=offset,
        scope=scope_filter,
        keyword=keyword,
        return_total=True,
    )
    total_pages = math.ceil(total / page_size) if total > 0 else 1

    col_p1, col_p2 = st.columns([1, 3])
    with col_p1:
        page = st.number_input(
            f"é¡µç  (å…± {total_pages} é¡µ)",
            min_value=1,
            max_value=max(1, total_pages),
            value=min(current_page, total_pages),
            key="central_page",
        )
    if page != current_page:
        offset = max(0, (page - 1) * page_size)
        items, total = db_mgr.browse_central_memories(
            limit=page_size,
            offset=offset,
            scope=scope_filter,
            keyword=keyword,
            return_total=True,
        )
    st.caption(f"æ€»è®°å½• {total}ï¼Œå½“å‰é¡µ {len(items)} æ¡")
    for item in items:
        with st.container(border=True):
            render_item(item)
            with st.expander("å…ƒæ•°æ®"):
                st.json(item.get("metadata", {}))

elif mode == "ğŸ”– å…¨å±€Tagsè°ƒè¯•":
    st.subheader("ğŸ”– å…¨å±€ Tags è°ƒè¯•ï¼ˆglobal_tags + memory_tag_rel + note_tag_relï¼‰")
    if not db_mgr.has_central_db():
        st.error("æœªæ‰¾åˆ°ä¸­å¤®è®°å¿†åº“ã€‚")
        st.stop()

    c1, c2, c3 = st.columns([3, 2, 2])
    with c1:
        query_text = st.text_input("è¾“å…¥åŸå§‹é—®é¢˜ï¼ˆç”¨äº tags å‘½ä¸­ï¼‰", value="")
    with c2:
        scope_for_hit = st.text_input("scopeï¼ˆå¯ç©ºï¼‰", value="")
    with c3:
        hit_limit = st.number_input("å‘½ä¸­è®°å¿†ä¸Šé™", min_value=1, max_value=200, value=30)

    if query_text:
        result = db_mgr.unified_tag_hit_search(query_text, limit=int(hit_limit), scope=scope_for_hit)
        st.markdown("**å‘½ä¸­ tags**")
        st.write(result.get("matched_tags", []))
        st.caption(f"matched_tag_ids: {result.get('matched_tag_ids', [])}")

        st.markdown("**å‘½ä¸­è®°å¿†**")
        hits = result.get("memory_hits", [])
        if hits:
            for item in hits:
                with st.container(border=True):
                    render_item(item, show_hit_count=True)
                    with st.expander("å…ƒæ•°æ®"):
                        st.json(item.get("metadata", {}))
        else:
            st.info("æ— å‘½ä¸­è®°å¿†ã€‚")

    st.divider()
    st.markdown("**å…¨å±€ tags åˆ—è¡¨**")
    tag_keyword = st.text_input("ç­›é€‰ tag name", value="", key="tag_keyword_filter")
    tags = db_mgr.get_global_tags(limit=300, offset=0, keyword=tag_keyword)
    st.caption(f"å…±è¿”å› {len(tags)} æ¡")
    if tags:
        st.dataframe(tags, use_container_width=True, hide_index=True)

elif mode == "ğŸ§­ memory_index æ£€ç´¢":
    st.subheader("ğŸ§­ å‘é‡è½»é‡ç´¢å¼•è°ƒè¯•ï¼ˆcollection = memory_indexï¼‰")
    if not db_mgr.has_vector_db():
        st.error("æœªè¿æ¥å‘é‡åº“ã€‚")
        st.stop()

    collections = db_mgr.get_collections()
    if "memory_index" not in collections:
        st.warning("æœªæ‰¾åˆ° `memory_index` é›†åˆã€‚")
    else:
        st.caption(f"memory_index count: {db_mgr.get_collection_stats('memory_index').get('count', 0)}")
        query = st.text_input("å‘é‡æŸ¥è¯¢", value="", placeholder="è¾“å…¥ä¸€å¥è¯æµ‹è¯•å‘é‡å¬å›")
        topk = st.slider("Top K", min_value=1, max_value=30, value=10)
        if query:
            results = db_mgr.query_collection(
                collection_name="memory_index",
                query_text=query,
                n_results=int(topk),
            )
            for item in results:
                if item.get("error"):
                    st.error(item.get("error"))
                    continue
                with st.container(border=True):
                    st.markdown(f"**score:** `{item.get('score', 0):.4f}` | **id:** `{item.get('id')}`")
                    st.code(item.get("document") or "", language="text")
                    if item.get("metadata"):
                        st.caption(f"metadata: {item.get('metadata')}")

        st.divider()
        st.markdown("**åŸå§‹æµè§ˆ**")
        page_size = 20
        page = st.number_input("é¡µç ", min_value=1, value=1, key="memory_index_page")
        offset = (int(page) - 1) * page_size
        items = db_mgr.browse_collection("memory_index", limit=page_size, offset=offset)
        for item in items:
            with st.container(border=True):
                st.markdown(f"**id:** `{item.get('id')}`")
                st.code(item.get("document") or "", language="text")
                if item.get("metadata"):
                    st.caption(f"metadata: {item.get('metadata')}")

elif mode == "ğŸ—‚ï¸ ä¸­å¤®ç¬”è®°ç´¢å¼•":
    st.subheader("ğŸ—‚ï¸ ä¸­å¤®ç¬”è®°ç´¢å¼•ï¼ˆnote_index_records + note_tag_relï¼‰")
    if not db_mgr.has_central_db():
        st.error("æœªæ‰¾åˆ°ä¸­å¤®è®°å¿†åº“ã€‚")
        st.stop()

    stats = db_mgr.get_note_index_stats()
    st.caption(
        f"note_index_records: {stats.get('note_index_count', 0)} | "
        f"note_tag_rel: {stats.get('note_tag_rel_count', 0)}"
    )
    keyword = st.text_input("å…³é”®è¯ï¼ˆè·¯å¾„/æ ‡é¢˜/tagsï¼‰", value="")
    page_size = 20
    current_page = int(st.session_state.get("note_index_page", 1) or 1)
    offset = max(0, (current_page - 1) * page_size)
    items, total = db_mgr.browse_note_index_records(
        limit=page_size,
        offset=offset,
        keyword=keyword,
        return_total=True,
    )
    total_pages = math.ceil(total / page_size) if total > 0 else 1
    page = st.number_input(
        f"é¡µç  (å…± {total_pages} é¡µ)",
        min_value=1,
        max_value=max(1, total_pages),
        value=min(current_page, total_pages),
        key="note_index_page",
    )
    if page != current_page:
        offset = max(0, (page - 1) * page_size)
        items, total = db_mgr.browse_note_index_records(
            limit=page_size,
            offset=offset,
            keyword=keyword,
            return_total=True,
        )
    st.caption(f"æ€»è®°å½• {total}ï¼Œå½“å‰é¡µ {len(items)} æ¡")
    for row in items:
        with st.container(border=True):
            title = " / ".join(
                [str(row.get(f"heading_h{i}") or "").strip() for i in range(1, 7) if str(row.get(f"heading_h{i}") or "").strip()]
            ) or "(æ— æ ‡é¢˜)"
            st.markdown(f"**{row.get('source_file_path', '')}**")
            st.caption(f"title: {title}")
            st.caption(
                f"note_short_id: {row.get('note_short_id', -1)} | total_lines: {row.get('total_lines', 0)} | "
                f"tags: {row.get('tags_text', '')}"
            )
            with st.expander("è¯¦æƒ…"):
                st.json(row)

elif mode == "ğŸ§  notes_index æ£€ç´¢":
    st.subheader("ğŸ§  å‘é‡è½»é‡ç´¢å¼•è°ƒè¯•ï¼ˆcollection = notes_indexï¼‰")
    if not db_mgr.has_vector_db():
        st.error("æœªè¿æ¥å‘é‡åº“ã€‚")
        st.stop()

    collections = db_mgr.get_collections()
    if "notes_index" not in collections:
        st.warning("æœªæ‰¾åˆ° `notes_index` é›†åˆã€‚")
    else:
        st.caption(f"notes_index count: {db_mgr.get_collection_stats('notes_index').get('count', 0)}")
        query = st.text_input("å‘é‡æŸ¥è¯¢", value="", placeholder="è¾“å…¥ä¸€å¥è¯æµ‹è¯•ç¬”è®°å‘é‡å¬å›")
        topk = st.slider("Top K", min_value=1, max_value=30, value=10, key="notes_index_topk")
        if query:
            results = db_mgr.query_collection(
                collection_name="notes_index",
                query_text=query,
                n_results=int(topk),
            )
            for item in results:
                if item.get("error"):
                    st.error(item.get("error"))
                    continue
                with st.container(border=True):
                    st.markdown(f"**score:** `{item.get('score', 0):.4f}` | **id:** `{item.get('id')}`")
                    st.code(item.get("document") or "", language="text")
                    if item.get("metadata"):
                        st.caption(f"metadata: {item.get('metadata')}")

elif mode == "ğŸ“ note_recall æ¨¡æ‹Ÿ":
    st.subheader("ğŸ“ note_recall æ¨¡æ‹Ÿï¼ˆæŒ‰ note_short_id + è¡ŒèŒƒå›´ï¼‰")
    if not os.path.exists(raw_dir):
        st.error(f"ç¬”è®°ç›®å½•ä¸å­˜åœ¨: {raw_dir}")
        st.stop()
    c1, c2, c3 = st.columns(3)
    with c1:
        note_short_id = st.number_input("note_short_id", min_value=0, value=0, step=1)
    with c2:
        start_line = st.number_input("start_line", min_value=1, value=1, step=1)
    with c3:
        end_line = st.number_input("end_line", min_value=1, value=200, step=1)

    if st.button("æ¨¡æ‹Ÿè¯»å–", key="simulate_note_recall"):
        row = db_mgr.get_note_index_by_short_id(int(note_short_id))
        if not row:
            st.error(f"æœªæ‰¾åˆ° note_short_id={int(note_short_id)}")
            st.stop()
        rel = str(row.get("source_file_path") or "").replace("\\", "/").strip().lstrip("/")
        target = os.path.abspath(os.path.join(raw_dir, rel.replace("/", os.sep)))
        raw_root = os.path.abspath(raw_dir)
        if not target.startswith(raw_root):
            st.error("source_file_path éæ³•ï¼ˆè¶Šç•Œè·¯å¾„ï¼‰")
        elif not os.path.exists(target):
            st.error(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{rel}")
        else:
            with open(target, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
            lines = text.splitlines()
            total_lines = len(lines)
            if start_line > end_line:
                st.error("start_line ä¸èƒ½å¤§äº end_line")
                st.stop()
            if total_lines <= 0:
                actual_start = 0
                actual_end = 0
                seg = ""
            else:
                actual_start = min(max(1, int(start_line)), total_lines)
                actual_end = min(max(1, int(end_line)), total_lines)
                if actual_start > actual_end:
                    actual_start = actual_end
                seg = "\n".join(lines[actual_start - 1: actual_end])
            st.caption(
                f"note_short_id={int(note_short_id)} | total_lines={total_lines} | "
                f"actual_start_line={actual_start} | actual_end_line={actual_end}"
            )
            st.code(seg, language="text")

elif mode == "ğŸ”„ ä¸­å¤®åº“å¯¼å…¥å¯¼å‡º":
    st.subheader("ğŸ”„ ä¸­å¤®è®°å¿†åº“ JSON å¯¼å…¥å¯¼å‡º")
    if not db_mgr.has_central_db():
        st.error("æœªæ‰¾åˆ°ä¸­å¤®è®°å¿†åº“ã€‚")
        st.stop()

    c1, c2 = st.columns(2)
    with c1:
        st.markdown("**å¯¼å‡ºä¸­å¤®å¿«ç…§**")
        if st.button("ç”Ÿæˆå¿«ç…§", key="btn_export_central"):
            st.session_state["central_snapshot"] = db_mgr.export_central_snapshot()
            st.success("å¿«ç…§å·²ç”Ÿæˆã€‚")
        snapshot = st.session_state.get("central_snapshot")
        if snapshot:
            t = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                "ä¸‹è½½ JSON",
                data=json.dumps(snapshot, ensure_ascii=False, indent=2),
                file_name=f"memory_center_snapshot_{t}.json",
                mime="application/json",
            )

    with c2:
        st.markdown("**å¯¼å…¥ JSON**")
        uploaded = st.file_uploader("é€‰æ‹© JSON æ–‡ä»¶", type=["json"])
        if uploaded is not None and st.button("æ‰§è¡Œå¯¼å…¥", key="btn_import_central"):
            try:
                payload = json.loads(uploaded.read().decode("utf-8"))
                stats = db_mgr.import_central_payload(payload)
                st.success(
                    f"å¯¼å…¥å®Œæˆï¼šæ–°å¢ {stats.get('inserted', 0)} / æ›´æ–° {stats.get('upserted', 0)} / "
                    f"è·³è¿‡ {stats.get('skipped', 0)} / å¤±è´¥ {stats.get('failed', 0)}"
                )
            except Exception as e:
                st.error(f"å¯¼å…¥å¤±è´¥ï¼š{e}")

elif mode == "ğŸ› ï¸ ç»´æŠ¤çŠ¶æ€":
    st.subheader("ğŸ› ï¸ ç»´æŠ¤çŠ¶æ€ä¸å¤‡ä»½")
    state = db_mgr.get_maintenance_state()
    if state:
        st.markdown("**maintenance_state.json**")
        st.json(state)
    else:
        st.warning("æœªæ‰¾åˆ° maintenance_state.json æˆ–æ–‡ä»¶ä¸ºç©ºã€‚")

    st.divider()
    st.markdown("**å¤‡ä»½æ–‡ä»¶ï¼ˆæœ€å¤šä¿ç•™3ä¸ªï¼‰**")
    backups = db_mgr.list_backups()
    if backups:
        st.dataframe(backups, use_container_width=True, hide_index=True)
        selected = st.selectbox(
            "é€‰æ‹©å¤‡ä»½é¢„è§ˆ",
            [b["name"] for b in backups],
            index=0,
        )
        selected_path = next((b["path"] for b in backups if b["name"] == selected), "")
        if selected_path:
            st.json(db_mgr.load_backup_preview(selected_path))
    else:
        st.info("æš‚æ— å¤‡ä»½æ–‡ä»¶ã€‚")

elif mode == "ğŸ“‚ æµè§ˆç¬”è®°æ–‡ä»¶":
    st.subheader("ğŸ“‚ raw ç›®å½•ç¬”è®°é¢„è§ˆï¼ˆä»…æ–‡ä»¶è§†å›¾ï¼‰")
    if not os.path.exists(raw_dir):
        st.error(f"ç¬”è®°ç›®å½•ä¸å­˜åœ¨: {raw_dir}")
        st.stop()

    md_files = []
    for root, _, files in os.walk(raw_dir):
        for f in files:
            if f.endswith(".md"):
                abs_path = os.path.join(root, f)
                rel_path = os.path.relpath(abs_path, raw_dir).replace("\\", "/")
                md_files.append(rel_path)
    md_files.sort()

    if not md_files:
        st.info("æœªå‘ç° markdown æ–‡ä»¶ã€‚")
        st.stop()

    selected = st.selectbox("é€‰æ‹©æ–‡ä»¶", md_files, index=0)
    full_path = os.path.join(raw_dir, selected.replace("/", os.sep))
    try:
        with open(full_path, "r", encoding="utf-8") as f:
            content = f.read()
        st.caption(selected)
        st.markdown(content)
    except Exception as e:
        st.error(f"è¯»å–å¤±è´¥: {e}")
