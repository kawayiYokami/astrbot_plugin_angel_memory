"""
åµŒå…¥å¼æ¨¡å‹æä¾›å•†æ¨¡å—

æä¾›ç»Ÿä¸€çš„åµŒå…¥æ¥å£æŠ½è±¡ï¼Œæ”¯æŒæœ¬åœ°æ¨¡å‹å’ŒAPIæä¾›å•†ã€‚
é€šè¿‡å·¥å‚æ¨¡å¼å®ç°æ™ºèƒ½é™çº§å’Œé…ç½®é©±åŠ¨ã€‚
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
import asyncio
import importlib
import subprocess
import threading
import sys
from collections import OrderedDict

# å°è¯•å¯¼å…¥astrbot loggerï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ ‡å‡†åº“logger
try:
    from astrbot.api import logger
except ImportError:
    import logging as logger_module

    logger = logger_module.getLogger(__name__)


class EmbeddingCache:
    """
    å‘é‡åµŒå…¥ç¼“å­˜ç±»

    ä½¿ç”¨LRUç­–ç•¥ï¼Œé™åˆ¶å†…å­˜å ç”¨åœ¨æŒ‡å®šå¤§å°ä»¥å†…ã€‚
    é€‚ç”¨äºå¹¶å‘åœºæ™¯ä¸‹ç›¸åŒæ–‡æœ¬çš„é‡å¤æŸ¥è¯¢ä¼˜åŒ–ã€‚
    """

    def __init__(self, max_memory_mb: float = 100.0, ttl_minutes: int = 30):
        """
        åˆå§‹åŒ–ç¼“å­˜

        Args:
            max_memory_mb: æœ€å¤§å†…å­˜å ç”¨ï¼ˆMBï¼‰ï¼Œé»˜è®¤100MB
            ttl_minutes: ç¼“å­˜é¡¹è¿‡æœŸæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤30åˆ†é’Ÿ
        """
        self._cache: OrderedDict[str, Dict[str, Any]] = OrderedDict()
        self._lock = threading.RLock()
        self._max_memory_bytes = int(max_memory_mb * 1024 * 1024)
        self._current_memory_bytes = 0
        self._hit_count = 0
        self._miss_count = 0
        self._ttl_seconds = ttl_minutes * 60  # è½¬æ¢ä¸ºç§’
        self._cleanup_threshold = 50  # æ¯æ¬¡è®¿é—®50é¡¹æ—¶æ‰§è¡Œä¸€æ¬¡æ¸…ç†æ£€æŸ¥

    def _estimate_size(self, text: str, embedding: List[float]) -> int:
        """
        ä¼°ç®—ç¼“å­˜é¡¹çš„å†…å­˜å¤§å°

        Args:
            text: æ–‡æœ¬å­—ç¬¦ä¸²
            embedding: å‘é‡

        Returns:
            ä¼°ç®—çš„å­—èŠ‚æ•°
        """
        # æ–‡æœ¬å¤§å°ï¼ˆPythonå­—ç¬¦ä¸²å¼€é”€çº¦ä¸º49å­—èŠ‚ + æ¯ä¸ªå­—ç¬¦ï¼‰
        text_size = sys.getsizeof(text)
        # å‘é‡å¤§å°ï¼ˆlistå¼€é”€ + æ¯ä¸ªfloat 8å­—èŠ‚ï¼‰
        embedding_size = sys.getsizeof(embedding) + len(embedding) * 8
        # æ—¶é—´æˆ³å’Œå…ƒæ•°æ®å¼€é”€ï¼ˆçº¦64å­—èŠ‚ï¼‰
        metadata_size = 64
        return text_size + embedding_size + metadata_size

    def _is_expired(self, cache_item: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥ç¼“å­˜é¡¹æ˜¯å¦è¿‡æœŸ

        Args:
            cache_item: ç¼“å­˜é¡¹å­—å…¸

        Returns:
            æ˜¯å¦è¿‡æœŸ
        """
        import time

        return time.time() - cache_item["timestamp"] > self._ttl_seconds

    def _cleanup_expired(self, force: bool = False):
        """
        æƒ°æ€§æ¸…ç†è¿‡æœŸç¼“å­˜é¡¹

        Args:
            force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†æ‰€æœ‰è¿‡æœŸé¡¹
        """
        import time

        if not force and self._hit_count % self._cleanup_threshold != 0:
            return

        expired_keys = []
        current_time = time.time()

        for key, cache_item in self._cache.items():
            if current_time - cache_item["timestamp"] > self._ttl_seconds:
                expired_keys.append(key)

        for key in expired_keys:
            cache_item = self._cache.pop(key)
            old_size = self._estimate_size(key, cache_item["embedding"])
            self._current_memory_bytes -= old_size

        if expired_keys:
            logger.debug(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")

    def get(self, text: str) -> Optional[List[float]]:
        """
        ä»ç¼“å­˜è·å–å‘é‡

        Args:
            text: æ–‡æœ¬

        Returns:
            å‘é‡ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸè¿”å›None
        """
        with self._lock:
            # æƒ°æ€§æ¸…ç†è¿‡æœŸé¡¹ï¼ˆæ¯50æ¬¡è®¿é—®æ£€æŸ¥ä¸€æ¬¡ï¼‰
            self._cleanup_expired()

            if text in self._cache:
                cache_item = self._cache[text]

                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if self._is_expired(cache_item):
                    # åˆ é™¤è¿‡æœŸé¡¹
                    old_size = self._estimate_size(text, cache_item["embedding"])
                    del self._cache[text]
                    self._current_memory_bytes -= old_size
                    self._miss_count += 1
                    return None

                self._hit_count += 1
                # ç§»åˆ°æœ€åï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
                self._cache.move_to_end(text)
                return cache_item["embedding"]
            else:
                self._miss_count += 1
                return None

    def put(self, text: str, embedding: List[float]) -> None:
        """
        å°†å‘é‡æ”¾å…¥ç¼“å­˜

        Args:
            text: æ–‡æœ¬
            embedding: å‘é‡
        """
        import time

        with self._lock:
            # å¦‚æœå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ—§çš„
            if text in self._cache:
                cache_item = self._cache[text]
                old_size = self._estimate_size(text, cache_item["embedding"])
                self._current_memory_bytes -= old_size
                del self._cache[text]

            # è®¡ç®—æ–°é¡¹å¤§å°
            new_size = self._estimate_size(text, embedding)

            # å¦‚æœå•ä¸ªé¡¹è¶…è¿‡æœ€å¤§å†…å­˜ï¼Œç›´æ¥è¿”å›ä¸ç¼“å­˜
            if new_size > self._max_memory_bytes:
                return

            # æ·˜æ±°æ—§é¡¹ç›´åˆ°æœ‰è¶³å¤Ÿç©ºé—´
            while (
                self._current_memory_bytes + new_size > self._max_memory_bytes
                and self._cache
            ):
                # åˆ é™¤æœ€æ—§çš„é¡¹ï¼ˆFIFOï¼‰
                oldest_key, oldest_value = self._cache.popitem(last=False)
                oldest_size = self._estimate_size(oldest_key, oldest_value["embedding"])
                self._current_memory_bytes -= oldest_size

            # æ·»åŠ æ–°é¡¹ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
            self._cache[text] = {"embedding": embedding, "timestamp": time.time()}
            self._current_memory_bytes += new_size

    def get_batch(
        self, texts: List[str]
    ) -> tuple[List[Optional[List[float]]], List[int]]:
        """
        æ‰¹é‡è·å–å‘é‡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            (ç¼“å­˜ç»“æœåˆ—è¡¨, æœªå‘½ä¸­çš„ç´¢å¼•åˆ—è¡¨)
            ç¼“å­˜ç»“æœä¸­ï¼Œå‘½ä¸­çš„æ˜¯å‘é‡ï¼Œæœªå‘½ä¸­çš„æ˜¯None
        """
        results = []
        missing_indices = []

        with self._lock:
            for i, text in enumerate(texts):
                # æ³¨æ„ï¼šgetæ–¹æ³•å·²ç»åŒ…å«TTLæ£€æŸ¥ï¼Œåœ¨é”å†…éƒ¨è°ƒç”¨å¯èƒ½å¯¼è‡´é‡å¤æ¸…ç†
                # ä¸ºäº†æ€§èƒ½ï¼Œç›´æ¥ä»_cacheæ£€æŸ¥
                cache_item = self._cache.get(text)
                if cache_item and not self._is_expired(cache_item):
                    results.append(cache_item["embedding"])
                    self._hit_count += 1
                    self._cache.move_to_end(text)
                else:
                    results.append(None)
                    self._miss_count += 1
                    if cache_item and self._is_expired(cache_item):
                        # æ¸…ç†è¿‡æœŸé¡¹
                        old_size = self._estimate_size(text, cache_item["embedding"])
                        del self._cache[text]
                        self._current_memory_bytes -= old_size
                    missing_indices.append(i)

        return results, missing_indices

    def put_batch(self, texts: List[str], embeddings: List[List[float]]) -> None:
        """
        æ‰¹é‡æ”¾å…¥ç¼“å­˜

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            embeddings: å‘é‡åˆ—è¡¨
        """
        for text, embedding in zip(texts, embeddings):
            self.put(text, embedding)

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self._cache.clear()
            self._current_memory_bytes = 0
            self._hit_count = 0
            self._miss_count = 0

    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç¼“å­˜ç»Ÿè®¡å­—å…¸
        """
        with self._lock:
            total_requests = self._hit_count + self._miss_count
            hit_rate = self._hit_count / total_requests if total_requests > 0 else 0.0

            # è®¡ç®—è¿‡æœŸé¡¹æ•°é‡
        import time

        current_time = time.time()
        expired_count = sum(
            1
            for item in self._cache.values()
            if current_time - item["timestamp"] > self._ttl_seconds
        )

        return {
            "cache_size": len(self._cache),
            "expired_items": expired_count,
            "memory_usage_mb": self._current_memory_bytes / (1024 * 1024),
            "max_memory_mb": self._max_memory_bytes / (1024 * 1024),
            "hit_count": self._hit_count,
            "miss_count": self._miss_count,
            "hit_rate": hit_rate,
            "total_requests": total_requests,
            "ttl_minutes": self._ttl_seconds / 60,
        }


class EmbeddingProvider(ABC):
    """åµŒå…¥æä¾›å•†æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ä¸ºæ–‡æ¡£åˆ—è¡¨ç”Ÿæˆå‘é‡åµŒå…¥

        Args:
            texts: æ–‡æ¡£å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å‘é‡åˆ—è¡¨ï¼Œæ¯ä¸ªå‘é‡æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨
        """
        pass

    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯

        Returns:
            åŒ…å«æ¨¡å‹è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """
        æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨

        Returns:
            æ˜¯å¦å¯ç”¨
        """
        pass

    @abstractmethod
    def get_provider_type(self) -> str:
        """
        è·å–æä¾›å•†ç±»å‹

        Returns:
            æä¾›å•†ç±»å‹æ ‡è¯†ç¬¦
        """
        pass

    @abstractmethod
    def shutdown(self):
        """å…³é—­æä¾›å•†ï¼Œé‡Šæ”¾èµ„æº"""
        pass


class LocalEmbeddingProvider(EmbeddingProvider):
    """æœ¬åœ°åµŒå…¥æ¨¡å‹æä¾›å•†ï¼ˆæ‡’åŠ è½½ï¼Œè‡ªåŠ¨ä¾èµ–å®‰è£…ï¼‰"""

    def __init__(
        self, model_name: str = "BAAI/bge-small-zh-v1.5", cache_size_mb: float = 100.0
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æä¾›å•†ï¼ˆæ‡’åŠ è½½æ¨¡å¼ï¼‰

        Args:
            model_name: æœ¬åœ°æ¨¡å‹åç§°
            cache_size_mb: ç¼“å­˜å¤§å°ï¼ˆMBï¼‰ï¼Œé»˜è®¤100MB
        """
        self.model_name = model_name
        self.logger = logger
        self._model = None
        self._model_class = None  # å»¶è¿Ÿå¯¼å…¥çš„SentenceTransformerç±»
        self._cache = EmbeddingCache(max_memory_mb=cache_size_mb)
        self._cache_enabled = True  # ç¼“å­˜å¯ç”¨æ ‡å¿—
        self._auto_install_attempted = False  # é¿å…é‡å¤å°è¯•è‡ªåŠ¨å®‰è£…

    def _ensure_dependencies(self):
        """ç¡®ä¿ä¾èµ–å·²å®‰è£…ï¼Œå¦‚éœ€è¦åˆ™è‡ªåŠ¨å®‰è£…"""
        if self._model_class is not None:
            return True  # å·²ç»åŠ è½½

        try:
            # å°è¯•å¯¼å…¥sentence_transformers
            self._model_class = importlib.import_module('sentence_transformers').SentenceTransformer
            self.logger.info("âœ… sentence-transformers å·²å®‰è£…")
            return True
        except ImportError:
            self.logger.warning("âš ï¸ sentence-transformers æœªå®‰è£…")

            # å¦‚æœå·²ç»å°è¯•è¿‡è‡ªåŠ¨å®‰è£…ï¼Œåˆ™ä¸å†é‡å¤å°è¯•
            if self._auto_install_attempted:
                self.logger.error("âŒ è‡ªåŠ¨å®‰è£…å·²å¤±è´¥ï¼Œè·³è¿‡")
                return False

            self._auto_install_attempted = True

            # è‡ªåŠ¨å®‰è£…ä¾èµ–
            self.logger.info("ğŸš€ è‡ªåŠ¨å®‰è£…æœ¬åœ°æ¨¡å‹ä¾èµ–...")
            try:
                subprocess.check_call([
                    sys.executable, "-m", "pip", "install",
                    "--upgrade",
                    "torch",
                    "sentence-transformers>=2.2.0"
                ])
                self.logger.info("âœ… æœ¬åœ°æ¨¡å‹ä¾èµ–å®‰è£…å®Œæˆ")

                # é‡æ–°å°è¯•å¯¼å…¥
                self._model_class = importlib.import_module('sentence_transformers').SentenceTransformer
                return True

            except subprocess.CalledProcessError as e:
                self.logger.error(f"âŒ è‡ªåŠ¨å®‰è£…å¤±è´¥: {e}")
                self.logger.error("è¯·æ‰‹åŠ¨å®‰è£…: pip install torch sentence-transformers")
                return False

    def _load_model(self):
        """æ‡’åŠ è½½æœ¬åœ°æ¨¡å‹"""
        if not self._ensure_dependencies():
            self.logger.error("âŒ æ— æ³•åŠ è½½æœ¬åœ°æ¨¡å‹ï¼šç¼ºå°‘ä¾èµ–")
            return

        try:
            self.logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹: {self.model_name}")
            self._model = self._model_class(self.model_name)
            self.logger.info(f"æœ¬åœ°åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ: {self.model_name}")
        except Exception as e:
            self.logger.error(f"æœ¬åœ°åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self._model = None

    def embed_documents_sync(self, texts: List[str]) -> List[List[float]]:
        """åŒæ­¥æ–¹æ³•ï¼šä¸ºæ–‡æ¡£åˆ—è¡¨ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if not texts:
            return []

        if not self.is_available():
            raise RuntimeError("æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨")

        # å¦‚æœç¼“å­˜å·²ç¦ç”¨ï¼Œç›´æ¥å¤„ç†ï¼ˆä½¿ç”¨å±€éƒ¨å˜é‡é¿å…å¹¶å‘é—®é¢˜ï¼‰
        cache = self._cache
        if not self._cache_enabled or not cache:
            embeddings = self._model.encode(texts, convert_to_numpy=True)
            return embeddings.tolist()

        # 1. å°è¯•ä»ç¼“å­˜è·å–
        cached_results, missing_indices = cache.get_batch(texts)

        # 2. å¦‚æœå…¨éƒ¨å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›
        if not missing_indices:
            self.logger.debug(f"âœ… ç¼“å­˜å…¨éƒ¨å‘½ä¸­ï¼Œè·³è¿‡å‘é‡åŒ–: {len(texts)}ä¸ªæ–‡æœ¬")
            return [r for r in cached_results if r is not None]

        # 3. å¯¹æœªå‘½ä¸­çš„æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
        missing_texts = [texts[i] for i in missing_indices]
        self.logger.debug(
            f"ğŸ”„ ç¼“å­˜éƒ¨åˆ†å‘½ä¸­ï¼Œéœ€è¦å‘é‡åŒ–: {len(missing_texts)}/{len(texts)}ä¸ªæ–‡æœ¬"
        )

        # ç›´æ¥åŒæ­¥è°ƒç”¨ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰
        new_embeddings = self._model.encode(missing_texts, convert_to_numpy=True)
        new_embeddings_list = new_embeddings.tolist()

        # 4. å°†æ–°å‘é‡å­˜å…¥ç¼“å­˜
        cache.put_batch(missing_texts, new_embeddings_list)

        # 5. åˆå¹¶ç»“æœï¼šç»„è£…å®Œæ•´çš„åµŒå…¥åˆ—è¡¨
        result = []
        new_embedding_iter = iter(new_embeddings_list)
        for cached in cached_results:
            if cached is not None:
                result.append(cached)
            else:
                result.append(next(new_embedding_iter))

        return result

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """å¼‚æ­¥æ–¹æ³•ï¼šä¸ºæ–‡æ¡£åˆ—è¡¨ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ–¹æ³•
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.embed_documents_sync, texts)

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.is_available():
            return {
                "model_name": self.model_name,
                "provider_type": "local",
                "status": "unavailable",
                "dimension": 0,
            }

        return {
            "model_name": self.model_name,
            "provider_type": "local",
            "status": "available",
            "dimension": self._model.get_sentence_embedding_dimension(),
            "max_sequence_length": getattr(self._model, "max_seq_length", None),
        }

    def is_available(self) -> bool:
        """æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨"""
        return self._model is not None

    def get_provider_type(self) -> str:
        """è·å–æä¾›å•†ç±»å‹"""
        return "local"

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self._cache.get_stats()

    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        if self._cache:
            self._cache.clear()
            self.logger.info("æœ¬åœ°æä¾›å•†ç¼“å­˜å·²æ¸…ç©º")

    def clear_and_disable_cache(self) -> None:
        """æ¸…ç†å¹¶ç¦ç”¨ç¼“å­˜ï¼ˆåˆå§‹åŒ–å®Œæˆåè°ƒç”¨ä»¥èŠ‚çœå†…å­˜ï¼‰"""
        cache = self._cache
        if cache:
            with cache._lock:
                stats = cache.get_stats()
                cache.clear()
                self._cache_enabled = False
                self._cache = None
                self.logger.info(
                    f"æœ¬åœ°æä¾›å•† {self.model_name} ç¼“å­˜å·²æ¸…ç†å¹¶ç¦ç”¨ "
                    f"(å‘½ä¸­ç‡: {stats.get('hit_rate', 0):.1%}, "
                    f"èŠ‚çœå†…å­˜: ~{stats.get('memory_usage_mb', 0):.1f}MB)"
                )

    def shutdown(self):
        """å…³é—­æœ¬åœ°æä¾›å•†"""
        self.clear_cache()
        self.logger.info(f"æœ¬åœ°åµŒå…¥æä¾›å•† {self.model_name} å·²å…³é—­")


class APIEmbeddingProvider(EmbeddingProvider):
    """APIåµŒå…¥æ¨¡å‹æä¾›å•†"""

    def __init__(self, provider, provider_id: str, cache_size_mb: float = 100.0):
        """
        åˆå§‹åŒ–APIåµŒå…¥æä¾›å•†

        Args:
            provider: AstrBotæä¾›å•†å¯¹è±¡
            provider_id: æä¾›å•†ID
            cache_size_mb: ç¼“å­˜å¤§å°ï¼ˆMBï¼‰ï¼Œé»˜è®¤100MB
        """
        self.provider = provider
        self.provider_id = provider_id
        self.logger = logger
        self._model_info = None
        self._available = None  # Noneè¡¨ç¤ºæœªæµ‹è¯•ï¼ŒTrue/Falseè¡¨ç¤ºæµ‹è¯•ç»“æœ
        self._cache = EmbeddingCache(max_memory_mb=cache_size_mb)
        self.batch_size = 64  # ç¨‹åºå¯åŠ¨æ—¶çš„æ‰¹é‡å¤§å°ï¼Œé‡åˆ°413ä¼šå‡åŠ
        self._cache_enabled = True  # ç¼“å­˜å¯ç”¨æ ‡å¿—

        # å»¶è¿Ÿæµ‹è¯•å¯ç”¨æ€§ï¼Œé¿å…åœ¨æ„é€ å‡½æ•°ä¸­è¿›è¡Œå¼‚æ­¥æ“ä½œ
        self.logger.info(
            f"APIåµŒå…¥æä¾›å•†å·²åˆå§‹åŒ–: {self.provider_id}, "
            f"æ‰¹é‡å¤§å°: {self.batch_size} (çº¯å¼‚æ­¥æ¨¡å¼)"
        )

    async def check_availability(self) -> bool:
        """å¼‚æ­¥æ£€æŸ¥å¯ç”¨æ€§"""
        if self._available is not None:
            return self._available

        try:
            # å°è¯•è·å–ä¸€ä¸ªç®€å•çš„åµŒå…¥æ¥æµ‹è¯•å¯ç”¨æ€§
            test_text = "test"
            await self._perform_embedding_test(test_text)
            self._available = True
            self.logger.info(f"APIåµŒå…¥æä¾›å•†å¯ç”¨: {self.provider_id}")
            return True
        except Exception as e:
            self.logger.warning(f"APIåµŒå…¥æä¾›å•†ä¸å¯ç”¨ {self.provider_id}: {e}")
            self._available = False
            return False

    async def _perform_embedding_test(self, text: str):
        """æ‰§è¡ŒåµŒå…¥æµ‹è¯•"""
        try:
            await self.provider.get_embedding(text)
        except Exception as e:
            raise e

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """å¼‚æ­¥æ–¹æ³•ï¼šä¸ºæ–‡æ¡£åˆ—è¡¨ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆå¸¦ç¼“å­˜ï¼Œæ‰¹æ¬¡å†…å»é‡ï¼Œè‡ªåŠ¨åˆ†æ‰¹ï¼‰"""
        if not texts:
            return []

        if not self.is_available():
            raise RuntimeError(f"APIæä¾›å•†ä¸å¯ç”¨: {self.provider_id}")

        # å¦‚æœç¼“å­˜å·²ç¦ç”¨ï¼Œç›´æ¥å¤„ç†ï¼ˆä½¿ç”¨å±€éƒ¨å˜é‡é¿å…å¹¶å‘é—®é¢˜ï¼‰
        cache = self._cache
        if not self._cache_enabled or not cache:
            unique_texts, original_to_unique, unique_to_original = self._deduplicate_texts(texts)
            unique_embeddings = await self._get_embeddings_with_batch(unique_texts, self.batch_size)
            return self._map_embeddings_back(unique_embeddings, unique_to_original, len(texts))

        # 1. å°è¯•ä»ç¼“å­˜è·å–
        cached_results, missing_indices = cache.get_batch(texts)

        # 2. å¦‚æœå…¨éƒ¨å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›
        if not missing_indices:
            return [r for r in cached_results if r is not None]

        # 3. è·å–æœªå‘½ä¸­çš„æ–‡æœ¬
        missing_texts = [texts[i] for i in missing_indices]

        # 4. æ‰¹æ¬¡å†…å»é‡
        unique_texts, original_to_unique, unique_to_original = self._deduplicate_texts(
            missing_texts
        )
        len(missing_texts) - len(unique_texts)

        # 5. ä½¿ç”¨å½“å‰æ‰¹é‡å¤§å°å¤„ç†å»é‡åçš„æ–‡æœ¬
        unique_embeddings = await self._get_embeddings_with_batch(
            unique_texts, self.batch_size
        )

        # 6. å°†å»é‡åçš„å‘é‡å›å¡«åˆ°åŸå§‹ä½ç½®
        full_missing_embeddings = self._map_embeddings_back(
            unique_embeddings, unique_to_original, len(missing_texts)
        )

        # 7. å°†å»é‡åçš„å‘é‡å­˜å…¥ç¼“å­˜
        cache.put_batch(unique_texts, unique_embeddings)

        # 8. åˆå¹¶ç»“æœï¼šç»„è£…å®Œæ•´çš„åµŒå…¥åˆ—è¡¨
        result = []
        new_embedding_iter = iter(full_missing_embeddings)
        for cached in cached_results:
            if cached is not None:
                result.append(cached)
            else:
                result.append(next(new_embedding_iter))

        return result

    def _deduplicate_texts(self, texts: List[str]) -> tuple:
        """
        æ–‡æœ¬å»é‡ï¼Œä¿æŒæ˜ å°„å…³ç³»

        Args:
            texts: å¾…å»é‡çš„æ–‡æœ¬åˆ—è¡¨

        Returns:
            tuple: (unique_texts, original_to_unique, unique_to_original)
                - unique_texts: å»é‡åçš„æ–‡æœ¬åˆ—è¡¨
                - original_to_unique: åŸå§‹ç´¢å¼•åˆ°å”¯ä¸€ç´¢å¼•çš„æ˜ å°„
                - unique_to_original: å”¯ä¸€æ–‡æœ¬å¯¹åº”çš„åŸå§‹ç´¢å¼•åˆ—è¡¨
        """
        unique_texts = []
        original_to_unique = {}  # åŸå§‹ç´¢å¼• -> å”¯ä¸€ç´¢å¼•
        unique_to_original = []  # å”¯ä¸€ç´¢å¼• -> [åŸå§‹ç´¢å¼•åˆ—è¡¨]

        for original_idx, text in enumerate(texts):
            if text not in unique_texts:
                # æ–°çš„å”¯ä¸€æ–‡æœ¬
                unique_idx = len(unique_texts)
                unique_texts.append(text)
                original_to_unique[original_idx] = unique_idx
                unique_to_original.append([original_idx])
            else:
                # é‡å¤æ–‡æœ¬ï¼Œæ‰¾åˆ°å¯¹åº”çš„å”¯ä¸€ç´¢å¼•
                unique_idx = unique_texts.index(text)
                original_to_unique[original_idx] = unique_idx
                unique_to_original[unique_idx].append(original_idx)

        return unique_texts, original_to_unique, unique_to_original

    def _map_embeddings_back(
        self,
        unique_embeddings: List[List[float]],
        unique_to_original: List[List[int]],
        original_count: int,
    ) -> List[List[float]]:
        """
        å°†å»é‡åçš„å‘é‡å›å¡«åˆ°åŸå§‹ä½ç½®

        Args:
            unique_embeddings: å»é‡æ–‡æœ¬çš„å‘é‡åˆ—è¡¨
            unique_to_original: å”¯ä¸€æ–‡æœ¬å¯¹åº”çš„åŸå§‹ç´¢å¼•åˆ—è¡¨
            original_count: åŸå§‹æ–‡æœ¬æ•°é‡

        Returns:
            List[List[float]]: å›å¡«åˆ°åŸå§‹ä½ç½®çš„å‘é‡åˆ—è¡¨
        """
        full_embeddings = [None] * original_count

        for unique_idx, original_indices in enumerate(unique_to_original):
            embedding = unique_embeddings[unique_idx]
            for original_idx in original_indices:
                full_embeddings[original_idx] = embedding

        return full_embeddings

    async def _get_embeddings_with_batch(
        self, texts: List[str], batch_size: int
    ) -> List[List[float]]:
        """ä½¿ç”¨æŒ‡å®šæ‰¹é‡å¤§å°å¹¶å‘è·å–å‘é‡åµŒå…¥"""

        if batch_size >= len(texts):
            # å•æ‰¹æ¬¡å¤„ç†
            result = await self.provider.get_embeddings(texts)
            return result
        else:
            # åˆ†æ‰¹å¹¶å‘å¤„ç†
            tasks = []
            batch_info = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i : i + batch_size]
                task = self.provider.get_embeddings(batch)
                tasks.append(task)
                batch_info.append(len(batch))

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
            batch_results = await asyncio.gather(*tasks)

            # æŒ‰é¡ºåºæ‹¼æ¥ç»“æœ
            result = []
            for batch_embeddings in batch_results:
                result.extend(batch_embeddings)

            return result

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self._model_info is None:
            meta = self.provider.meta() if hasattr(self.provider, "meta") else {}
            self._model_info = {
                "provider_id": self.provider_id,
                "provider_type": "api",
                "status": "available" if self._available else "unavailable",
                "meta": meta,
            }

        return self._model_info

    def is_available(self) -> bool:
        """æ£€æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨"""
        return self._available is True

    def get_provider_type(self) -> str:
        """è·å–æä¾›å•†ç±»å‹"""
        return "api"

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self._cache.get_stats()
        stats["current_batch_size"] = self.batch_size  # æ·»åŠ å½“å‰æ‰¹é‡å¤§å°ä¿¡æ¯
        return stats

    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        if self._cache:
            self._cache.clear()
            self.logger.info(f"APIæä¾›å•† {self.provider_id} ç¼“å­˜å·²æ¸…ç©º")

    def clear_and_disable_cache(self) -> None:
        """æ¸…ç†å¹¶ç¦ç”¨ç¼“å­˜ï¼ˆåˆå§‹åŒ–å®Œæˆåè°ƒç”¨ä»¥èŠ‚çœå†…å­˜ï¼‰"""
        cache = self._cache
        if cache:
            with cache._lock:
                stats = cache.get_stats()
                cache.clear()
                self._cache_enabled = False
                self._cache = None
                self.logger.info(
                    f"APIæä¾›å•† {self.provider_id} ç¼“å­˜å·²æ¸…ç†å¹¶ç¦ç”¨ "
                    f"(å‘½ä¸­ç‡: {stats.get('hit_rate', 0):.1%}, "
                    f"èŠ‚çœå†…å­˜: ~{stats.get('memory_usage_mb', 0):.1f}MB)"
                )

    def shutdown(self):
        """å…³é—­APIæä¾›å•†ï¼Œé‡Šæ”¾èµ„æº"""
        self.logger.info(f"æ­£åœ¨å…³é—­APIåµŒå…¥æä¾›å•†: {self.provider_id}")
        self.clear_cache()
        self.logger.info(f"APIåµŒå…¥æä¾›å•† {self.provider_id} å·²æˆåŠŸå…³é—­")


class EmbeddingProviderFactory:
    """åµŒå…¥æä¾›å•†å·¥å‚"""

    def __init__(self, context=None):
        """
        åˆå§‹åŒ–å·¥å‚

        Args:
            context: AstrBotä¸Šä¸‹æ–‡ï¼Œç”¨äºè·å–æä¾›å•†
        """
        self.context = context
        self.logger = logger

    async def create_provider(
        self,
        provider_id: Optional[str] = None,
        local_model_name: str = "BAAI/bge-small-zh-v1.5",
        enable_local_embedding: bool = True,
    ) -> EmbeddingProvider:
        """
        åˆ›å»ºåµŒå…¥æä¾›å•†

        Args:
            provider_id: APIæä¾›å•†IDï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            local_model_name: æœ¬åœ°æ¨¡å‹åç§°
            enable_local_embedding: æ˜¯å¦å¯ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹

        Returns:
            åµŒå…¥æä¾›å•†å®ä¾‹
        """
        # å¯ç”¨æœ¬åœ°æ¨¡å‹æ—¶ç›´æ¥è¿”å›
        if enable_local_embedding:
            self.logger.info("ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹")
            return LocalEmbeddingProvider(local_model_name)

        # APIæ¨¡å¼ä¸‹å¿…é¡»æä¾›provider_id
        if not provider_id:
            raise ValueError(
                "é”™è¯¯ï¼šåµŒå…¥æ¨¡å‹æä¾›å•†ID (provider_id) ä¸ºç©ºï¼Œä¸”æœ¬åœ°åµŒå…¥å·²ç¦ç”¨ (enable_local_embedding=False)ã€‚\n"
                "è§£å†³æ–¹æ¡ˆï¼š\n"
                "1. åœ¨é…ç½®ä¸­è®¾ç½® 'astrbot_embedding_provider_id' ä¸ºæœ‰æ•ˆçš„APIæä¾›å•†IDï¼Œæˆ–\n"
                "2. å°† 'enable_local_embedding' è®¾ç½®ä¸º True ä»¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œæˆ–\n"
                "3. åœ¨ç³»ç»Ÿä¸­æ³¨å†ŒåµŒå…¥æä¾›å•†å¹¶é…ç½®å…¶IDã€‚"
            )

        # å°è¯•ä½¿ç”¨APIæä¾›å•†
        if not self.context:
            raise Exception("æ— ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ— æ³•è·å–APIæä¾›å•†")

        provider = self.context.get_provider_by_id(provider_id)
        if not provider:
            raise Exception(f"æœªæ‰¾åˆ°APIæä¾›å•†: {provider_id}")

        api_provider = APIEmbeddingProvider(provider, provider_id)
        if not await api_provider.check_availability():
            raise Exception(f"APIæä¾›å•†ä¸å¯ç”¨: {provider_id}")

        self.logger.info(f"æˆåŠŸä½¿ç”¨APIåµŒå…¥æä¾›å•†: {provider_id}")
        return api_provider

    def get_available_providers(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„æä¾›å•†ä¿¡æ¯

        Returns:
            å¯ç”¨æä¾›å•†ä¿¡æ¯åˆ—è¡¨
        """
        providers = []

        # æ·»åŠ æœ¬åœ°æ¨¡å‹ä¿¡æ¯
        local_provider = LocalEmbeddingProvider()
        providers.append(local_provider.get_model_info())

        # æ·»åŠ APIæä¾›å•†ä¿¡æ¯
        if self.context and hasattr(self.context, "get_all_embedding_providers"):
            try:
                api_providers = self.context.get_all_embedding_providers()
                for provider in api_providers:
                    try:
                        meta = provider.meta() if hasattr(provider, "meta") else {}
                        provider_info = {
                            "provider_id": meta.get("id", "unknown"),
                            "provider_type": "api",
                            "status": "available",
                            "meta": meta,
                        }
                        providers.append(provider_info)
                    except Exception as e:
                        self.logger.warning(f"è·å–APIæä¾›å•†ä¿¡æ¯å¤±è´¥: {e}")
            except Exception as e:
                self.logger.warning(f"è·å–APIæä¾›å•†åˆ—è¡¨å¤±è´¥: {e}")

        return providers
