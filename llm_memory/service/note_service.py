"""
ç¬”è®°æœåŠ¡ - LLMçš„ç¬”è®°ç®¡ç†æœåŠ¡

æä¾›ç¬”è®°çš„å¢åˆ æ”¹æŸ¥åŠŸèƒ½ï¼Œä¸è®°å¿†ç³»ç»Ÿå…±äº«åŒä¸€ä¸ªå‘é‡æ•°æ®åº“å®ä¾‹ã€‚
æ”¯æŒç›®å½•è§£æã€æ–‡æ¡£å‘é‡åŒ–å­˜å‚¨å’Œæ™ºèƒ½æŸ¥è¯¢ã€‚
"""

import asyncio
import concurrent.futures
from typing import List, Dict
from pathlib import Path

from ..models.note_models import NoteData
from ..parser.parser_manager import parser_manager
from .id_service import IDService

# å¯¼å…¥æ—¥å¿—è®°å½•å™¨
try:
    from astrbot.api import logger
except ImportError:
    import logging
    logger = logging.getLogger(__name__)
from ..config.system_config import system_config


class NoteServiceError(Exception):
    """ç¬”è®°æœåŠ¡å¼‚å¸¸åŸºç±»"""

    pass


class NoteNotFoundError(NoteServiceError):
    """ç¬”è®°æœªæ‰¾åˆ°å¼‚å¸¸"""

    pass


class NoteOperationError(NoteServiceError):
    """ç¬”è®°æ“ä½œå¤±è´¥å¼‚å¸¸"""

    pass


class NoteService:
    """
    ç¬”è®°æœåŠ¡ç±»

    æä¾›LLMç¬”è®°çš„å®Œæ•´ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - ç¬”è®°çš„æ·»åŠ ã€åˆ é™¤ã€ä¿®æ”¹ã€æŸ¥è¯¢
    - ç›®å½•è§£æå’Œæ–‡æ¡£å‘é‡åŒ–
    - æ™ºèƒ½æ ‡ç­¾æå–
    - è¯­ä¹‰æœç´¢
    - ä¸è®°å¿†ç³»ç»Ÿçš„æ•°æ®éš”ç¦»
    - æ‰¹é‡å‘é‡åŒ–ä¼˜åŒ–
    """

    def __init__(self, plugin_context=None, vector_store=None):
        """
        åˆå§‹åŒ–ç¬”è®°æœåŠ¡ã€‚

        Args:
            plugin_context: PluginContextæ’ä»¶ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            vector_store: VectorStoreå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼Œå¦‚æœæœªæä¾›plugin_contextåˆ™å¿…éœ€ï¼‰
        """
        self.logger = logger
        self.plugin_context = plugin_context

        # ä¼˜å…ˆä½¿ç”¨PluginContextï¼Œå¦åˆ™ä½¿ç”¨ä¼ å…¥çš„vector_store
        if plugin_context:
            # ä»PluginContextåˆ›å»ºIDService
            self.id_service = IDService.from_plugin_context(plugin_context)
            # vector_storeéœ€è¦åœ¨ComponentFactoryä¸­è®¾ç½®
            self.vector_store = None
            self.collections_initialized = False
            self.logger.info("ç¬”è®°æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆPluginContextæ¨¡å¼ï¼‰ï¼Œç­‰å¾…vector_storeè®¾ç½®ã€‚")
        elif vector_store:
            # å‘åå…¼å®¹æ¨¡å¼
            if not vector_store:
                raise ValueError("å¿…é¡»æä¾›ä¸€ä¸ª VectorStore å®ä¾‹ã€‚")
            self.vector_store = vector_store

            # åˆå§‹åŒ–IDæœåŠ¡ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            self.id_service = IDService()

            # é€šè¿‡VectorStoreè·å–é›†åˆ
            self._initialize_collections()
            self.logger.info("ç¬”è®°æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰ï¼Œå·²å»ºç«‹ä¸“ç”¨çš„ä¸»å‰¯é›†åˆã€‚")
        else:
            raise ValueError("å¿…é¡»æä¾› plugin_context æˆ– vector_store å‚æ•°")

        # è·å–è§£æå™¨ç®¡ç†å™¨
        self.parser_manager = parser_manager

        # åˆ›å»ºè½»é‡çº¿ç¨‹æ± ï¼ˆä»…ç”¨äºåŒæ­¥è§£æå™¨çš„å…¼å®¹ï¼Œé¡ºåºå¤„ç†æ¨¡å¼ä¸‹å‡ ä¹ä¸å¹¶å‘ï¼‰
        self._thread_pool = concurrent.futures.ThreadPoolExecutor(
            max_workers=1,  # é¡ºåºå¤„ç†ï¼Œåªéœ€1ä¸ªçº¿ç¨‹
            thread_name_prefix="NoteService"
        )

        # æ‰¹é‡å‘é‡åŒ–ä¼˜åŒ–é…ç½®
        self._batch_size = 64  # æ¯æ‰¹æ¬¡æœ€å¤š64ä¸ªæ–‡æ¡£å—
        self._batch_timeout = 5.0  # æœ€å¤§ç­‰å¾…5ç§’
        self._embedding_queue = []  # å¾…å‘é‡åŒ–çš„é˜Ÿåˆ—
        self._batch_lock = asyncio.Lock()  # å¼‚æ­¥é”ä¿æŠ¤é˜Ÿåˆ—
        self.logger.info("NoteServiceåˆå§‹åŒ–å®Œæˆï¼ˆé¡ºåºå¤„ç†æ¨¡å¼ï¼‰")

    def _initialize_collections(self):
        """åˆå§‹åŒ–ChromaDBé›†åˆ"""
        if not self.vector_store:
            raise ValueError("VectorStoreæœªè®¾ç½®ï¼Œæ— æ³•åˆå§‹åŒ–é›†åˆ")

        # é€šè¿‡VectorStoreè·å–é›†åˆï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶é¿å…é‡å¤åˆå§‹åŒ–
        self.main_collection = (
            self.vector_store.get_or_create_collection_with_dimension_check(
                system_config.notes_main_collection_name
            )
        )
        self.sub_collection = (
            self.vector_store.get_or_create_collection_with_dimension_check(
                system_config.notes_sub_collection_name
            )
        )
        self.collections_initialized = True

    def set_vector_store(self, vector_store):
        """
        è®¾ç½®VectorStoreå®ä¾‹ï¼ˆç”¨äºPluginContextæ¨¡å¼ï¼‰

        Args:
            vector_store: VectorStoreå®ä¾‹
        """
        if self.vector_store:
            self.logger.warning("VectorStoreå·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–")

        self.vector_store = vector_store
        self._initialize_collections()
        self.logger.info("VectorStoreå·²è®¾ç½®ï¼Œé›†åˆåˆå§‹åŒ–å®Œæˆ")

    def ensure_ready(self):
        """ç¡®ä¿æœåŠ¡å·²å‡†å¤‡å°±ç»ªï¼ˆç”¨äºPluginContextæ¨¡å¼ï¼‰"""
        if not self.collections_initialized:
            if not self.vector_store:
                raise RuntimeError("NoteServiceæœªè®¾ç½®VectorStoreï¼Œè¯·å…ˆè°ƒç”¨set_vector_store()")
            self._initialize_collections()
        return True

    def search_notes(
        self, query: str, max_results: int = 10, tag_filter: List[str] = None, threshold: float = 0.5
    ) -> List[Dict]:
        """
        æœç´¢ç¬”è®°

        Args:
            query: æŸ¥è¯¢å†…å®¹
            max_results: æœ€å¤§ç»“æœæ•°
            tag_filter: æ ‡ç­¾è¿‡æ»¤
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰ï¼Œä½äºæ­¤é˜ˆå€¼çš„ç»“æœå°†è¢«è¿‡æ»¤

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ç¡®ä¿æœåŠ¡å·²å‡†å¤‡å°±ç»ª
            self.ensure_ready()
            # ä½¿ç”¨ä¸¤é˜¶æ®µæ··åˆæ£€ç´¢ç­–ç•¥ï¼Œä¼ é€’é˜ˆå€¼å‚æ•°
            results = self._hybrid_search(query, max_results, threshold=threshold)
            return results

        except Exception as e:
            self.logger.error(f"æœç´¢ç¬”è®°å¤±è´¥: {e}")
            return []

    def search_notes_by_token_limit(
        self, query: str, max_tokens: int = 10000, recall_count: int = 100, tag_filter: List[str] = None
    ) -> List[Dict]:
        """
        åŸºäºtokenæ•°é‡é™åˆ¶æœç´¢ç¬”è®°

        Args:
            query: æŸ¥è¯¢å†…å®¹
            max_tokens: æœ€å¤§tokenæ•°é™åˆ¶
            recall_count: å€™é€‰ç»“æœæ•°é‡
            tag_filter: æ ‡ç­¾è¿‡æ»¤

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼ˆä¿æŒç›¸å…³æ€§æ’åºï¼ŒæŒ‰tokené™åˆ¶åŠ¨æ€æˆªå–ï¼‰
        """
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ··åˆæ£€ç´¢ç®—æ³•è·å–é«˜è´¨é‡æ’åºç»“æœ
            candidates = self._hybrid_search(query, recall_count=recall_count, max_results=recall_count)

            # å¦‚æœå€™é€‰ç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å›
            if not candidates:
                return []

            # æŒ‰tokené™åˆ¶åŠ¨æ€é€‰æ‹©ç»“æœ
            from ..utils.token_utils import count_tokens

            selected_notes = []
            current_tokens = 0
            candidate_count = 0

            for note in candidates:
                note_content = note.get('content', '')
                note_tokens = count_tokens(note_content)

                # æ£€æŸ¥æ˜¯å¦è¶…å‡ºtokené™åˆ¶
                if current_tokens + note_tokens <= max_tokens:
                    selected_notes.append(note)
                    current_tokens += note_tokens
                    candidate_count += 1
                else:
                    # åŠ ä¸Šè¿™ä¸ªç¬”è®°ä¼šè¶…å‡ºtokené™åˆ¶ï¼Œåœæ­¢æ·»åŠ 
                    self.logger.debug(
                        f"Tokené™åˆ¶({max_tokens})è¾¾åˆ°ï¼Œé€‰æ‹©{candidate_count}ä¸ªç¬”è®°ï¼Œå…±{current_tokens} tokens"
                    )
                    break

            self.logger.debug(
                f"åŸºäºtokené™åˆ¶æœç´¢å®Œæˆ: è¿”å›{len(selected_notes)}ä¸ªç¬”è®°ï¼Œå…±{current_tokens} tokensï¼Œä»{len(candidates)}ä¸ªå€™é€‰ä¸­é€‰å‡º"
            )
            return selected_notes

        except Exception as e:
            self.logger.error(f"åŸºäºtokené™åˆ¶æœç´¢ç¬”è®°å¤±è´¥: {e}")
            return []

    def _hybrid_search(
        self,
        query: str,
        max_results: int = 10,
        recall_count: int = 100,
        threshold: float = 0.5,
    ) -> List[Dict]:
        """
        ä¸¤é˜¶æ®µæ··åˆæ£€ç´¢: å…ˆè¿‡æ»¤ï¼Œåé‡æ’ã€‚

        Args:
            query: æŸ¥è¯¢å†…å®¹
            max_results: æœ€å¤§ç»“æœæ•°
            recall_count: ç¬¬ä¸€é˜¶æ®µå¬å›æ•°é‡
            threshold: å†…å®¹ç›¸ä¼¼åº¦è¿‡æ»¤é˜ˆå€¼

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """

        # 1. ç¬¬ä¸€é˜¶æ®µï¼šè¿‡æ»¤ (Filtering)
        # ä½¿ç”¨ VectorStore çš„ç¬”è®°ä¸“ç”¨æ£€ç´¢æ–¹æ³•è¿›è¡Œå‘é‡æœç´¢
        recall_results = self.vector_store.search_notes(
            collection=self.main_collection,
            query=query,
            limit=recall_count
        )

        # å¤„ç†æ— ç»“æœæƒ…å†µ
        if not recall_results:
            return []

        # æ„å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å¬å›ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œå¹¶åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        all_recalled_notes = []
        for note in recall_results:
            # è·å–çœŸå®çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç”±VectorStoreè®¾ç½®ï¼‰
            content_similarity = getattr(note, 'similarity', 0.0)

            # åº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼šåªä¿ç•™ç›¸ä¼¼åº¦ >= threshold çš„ç»“æœ
            if content_similarity < threshold:
                continue

            # ä» NoteData å¯¹è±¡ä¸­æå–æ•°æ®
            tag_names = self.id_service.ids_to_tags(note.tag_ids)
            all_recalled_notes.append(
                {
                    "id": note.id,
                    "content": note.content,
                    "metadata": note.to_dict(),
                    "tags": tag_names,  # è½¬æ¢ä¸ºtag_names
                    "content_similarity": content_similarity,  # ä½¿ç”¨çœŸå®ç›¸ä¼¼åº¦
                }
            )

        # å¦‚æœæ²¡æœ‰ç¬”è®°é€šè¿‡é˜ˆå€¼è¿‡æ»¤ï¼Œç›´æ¥è¿”å›
        if not all_recalled_notes:
            self.logger.debug(f"æ‰€æœ‰å¬å›ç»“æœéƒ½ä½äºé˜ˆå€¼ {threshold}ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        # 2. ç¬¬äºŒé˜¶æ®µï¼šé‡æ’ (Reranking)
        # ä½¿ç”¨è½»é‡çº§æ–¹æ³•æŸ¥è¯¢å‰¯é›†åˆï¼ˆåªè·å–æ ‡ç­¾ç›¸å…³æ€§åˆ†æ•°ï¼‰
        # å‰¯é›†åˆä¸å­˜å‚¨ metadataï¼Œæ‰€ä»¥ä¸èƒ½ä½¿ç”¨ search_notes
        tag_scores = self.vector_store._search_vector_scores(
            collection=self.sub_collection,
            query=query,
            limit=len(all_recalled_notes)
        )

        # 3. æœ€ç»ˆæ’åº
        # å°†æ ‡ç­¾åˆ†æ•°é™„åŠ åˆ°é€šè¿‡ç¬¬ä¸€é˜¶æ®µçš„ç¬”è®°ä¸Š
        for note in all_recalled_notes:
            note["tag_score"] = tag_scores.get(
                note["id"], 0.0
            )  # å¦‚æœåœ¨å‰¯é›†åˆä¸­æ²¡æ‰¾åˆ°ï¼Œåˆ™æ ‡ç­¾åˆ†ä¸º0

        # æ ¹æ®æ ‡ç­¾åˆ†æ•°è¿›è¡Œé™åºæ’åº
        all_recalled_notes.sort(key=lambda x: x["tag_score"], reverse=True)

        # æ–‡æ¡£å»é‡ï¼šåœ¨åŒä¸€æ–‡æ¡£ä¸­åªä¿ç•™æ’åæœ€é«˜çš„ç‰‡æ®µ
        # ç”±äºå·²æŒ‰æ ‡ç­¾åˆ†æ•°æ’åºï¼Œç¬¬ä¸€æ¬¡å‡ºç°çš„å°±æ˜¯è¯¥æ–‡æ¡£ä¸­æ ‡ç­¾æœ€ç›¸å…³çš„å—
        seen_file_ids = set()
        deduplicated_notes = []

        for note in all_recalled_notes:
            file_id = note["metadata"].get("file_id")
            if file_id is None:
                self.logger.warning(f"Note {note['id']} is missing 'file_id' in metadata, skipping deduplication for this item.")
                deduplicated_notes.append(note)
                continue

            if file_id not in seen_file_ids:
                seen_file_ids.add(file_id)
                deduplicated_notes.append(note)

        # 4. ç»„è£…æœ€ç»ˆç»“æœ
        # æˆªå–æ‰€éœ€æ•°é‡çš„ç»“æœï¼Œå¹¶æ ¼å¼åŒ–è¾“å‡º
        final_results = []
        for note in deduplicated_notes[:max_results]:
            final_results.append(
                {
                    "id": note["id"],
                    "content": note["content"],
                    "metadata": note["metadata"],
                    "tags": note["tags"],
                    "similarity": note[
                        "content_similarity"
                    ],  # è¿”å›å†…å®¹ç›¸ä¼¼åº¦ï¼Œå› ä¸ºè¿™æ›´ç›´è§‚
                }
            )

        return final_results

    def get_note(self, note_id: str) -> Dict:
        """
        è·å–æŒ‡å®šç¬”è®°

        Args:
            note_id: ç¬”è®°ID

        Returns:
            ç¬”è®°å†…å®¹å­—å…¸

        Raises:
            NoteNotFoundError: å½“ç¬”è®°ä¸å­˜åœ¨æ—¶
            NoteOperationError: å½“è·å–è¿‡ç¨‹ä¸­å‘ç”Ÿå…¶ä»–é”™è¯¯æ—¶
        """
        try:
            # ä¸»è¦ä¿¡æ¯ä»ä¸»é›†åˆè·å–
            results = self.main_collection.get(ids=[note_id])

            if results and results["ids"] and results["ids"][0]:
                # ç»„è£…è¿”å›æ•°æ®
                metadata = results["metadatas"][0] if results["metadatas"] else {}
                document = metadata.get("content", "")

                # ä»tag_idsè½¬æ¢ä¸ºtag_names
                tag_ids = metadata.get("tag_ids", [])
                if isinstance(tag_ids, str):
                    import json
                    try:
                        tag_ids = json.loads(tag_ids)
                    except json.JSONDecodeError:
                        tag_ids = []

                # åˆ›å»ºNoteDataå¯¹è±¡ä»¥è·å–tag_names
                note_data = NoteData.from_dict(metadata)
                tag_names = self.id_service.ids_to_tags(note_data.tag_ids)

                formatted_note = {
                    "id": note_id,
                    "content": document,
                    "tags": tag_names,
                    "metadata": metadata,
                }

                return formatted_note
            else:
                self.logger.warning(f"ç¬”è®°ä¸å­˜åœ¨: {note_id}")
                raise NoteNotFoundError(f"ç¬”è®°ä¸å­˜åœ¨: {note_id}")

        except NoteServiceError:
            # é‡æ–°æŠ›å‡ºæˆ‘ä»¬è‡ªå·±çš„å¼‚å¸¸
            raise
        except Exception as e:
            self.logger.error(f"è·å–ç¬”è®°å¤±è´¥: {e}")
            raise NoteOperationError(f"è·å–ç¬”è®°å¤±è´¥: {e}") from e

    # ===== æ–‡ä»¶è§£æå’Œæ–‡æ¡£å‘é‡åŒ– =====

    def parse_and_store_file_sync(self, file_path: str, relative_path: str = None) -> tuple:
        """
        åŒæ­¥ç‰ˆæœ¬ï¼šè§£æå¹¶å­˜å‚¨æ–‡ä»¶ï¼ˆé¡ºåºå¤„ç†ä¼˜åŒ–ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            relative_path: ç›¸å¯¹è·¯å¾„

        Returns:
            (æ–‡æ¡£æ•°é‡, è®¡æ—¶å­—å…¸)
        """
        import time
        timings = {}

        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_path_obj = Path(file_path)
            if not file_path_obj.exists():
                self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return 0, timings

            # è·å–è§£æå™¨
            t_parser = time.time()
            parser = self.parser_manager.get_parser_for_file(file_path, self.id_service.tag_manager)
            timings['parser_select'] = (time.time() - t_parser) * 1000

            if parser is None:
                self.logger.warning(f"æœªæ‰¾åˆ°é€‚åˆçš„è§£æå™¨ï¼Œè·³è¿‡å¤„ç†: {file_path}")
                return 0, timings

            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_path_obj = Path(file_path)
            file_timestamp = int(file_path_obj.stat().st_mtime)

            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„relative_path
            if relative_path is None:
                relative_path = file_path_obj.name

            # è·å–æˆ–åˆ›å»ºæ–‡ä»¶ID
            t_start = time.time()
            file_id = self.id_service.file_to_id(relative_path, file_timestamp)
            timings['id_lookup'] = (time.time() - t_start) * 1000

            # åŒæ­¥è§£ææ–‡ä»¶
            t_start = time.time()
            document_blocks = self._parse_file_sync(file_path, parser, file_id)
            timings['parse'] = (time.time() - t_start) * 1000

            # å­˜å‚¨notesï¼ˆå®Œå…¨åŒæ­¥ç‰ˆæœ¬ï¼Œç›´æ¥å­˜å‚¨ä¸èµ°é˜Ÿåˆ—ï¼‰
            if document_blocks:
                t_store_submit = time.time()

                # ç›´æ¥åŒæ­¥å­˜å‚¨ï¼ˆè·³è¿‡æ‰¹é‡é˜Ÿåˆ—ï¼Œä¸æ›´æ–°BM25ï¼‰
                store_timings = self._store_notes_batch(document_blocks, update_bm25=False)

                timings['store_total'] = (time.time() - t_store_submit) * 1000

                if store_timings:
                    timings.update(store_timings)
            else:
                timings['store_total'] = 0

            return len(document_blocks), timings

        except Exception as e:
            self.logger.error(f"åŒæ­¥è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            raise

    def _parse_file_sync(self, file_path: str, parser, file_id: int) -> List[NoteData]:
        """
        åŒæ­¥è§£ææ–‡ä»¶çš„è¾…åŠ©æ–¹æ³•

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            parser: è§£æå™¨å®ä¾‹
            file_id: æ–‡ä»¶ç´¢å¼•ID

        Returns:
            ç¬”è®°æ•°æ®åˆ—è¡¨
        """
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
            except UnicodeDecodeError:
                # å¯¹äºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¼ é€’ç©ºå†…å®¹ï¼Œç”±è§£æå™¨å¤„ç†
                content = ""

            # è§£ææ–‡æ¡£ï¼Œä¼ é€’file_idï¼ˆè§£æå™¨å·²ç»æœ‰TagManagerï¼‰
            return parser.parse(content, file_id, file_path)
        except Exception as e:
            self.logger.error(f"åŒæ­¥è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            raise

    def _is_supported_file(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒè§£æ

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦æ”¯æŒ
        """
        path = Path(file_path)
        extension = path.suffix.lower()
        return self.parser_manager.is_supported_extension(extension)

    def _store_notes_batch(self, notes: List[NoteData], update_bm25: bool = False) -> dict:
        """
        æ‰¹é‡å­˜å‚¨ç¬”è®°åˆ°å‘é‡æ•°æ®åº“ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰

        å­˜å‚¨ç­–ç•¥ï¼š
        - ä¸»é›†åˆï¼šå­˜å‚¨å®Œæ•´ç¬”è®°ä¿¡æ¯ï¼ˆvector åŸºäº"å†…å®¹+æ ‡ç­¾"ï¼Œmetadata åŒ…å«æ‰€æœ‰æ•°æ®ï¼‰
        - å‰¯é›†åˆï¼šä»…å­˜å‚¨æ ‡ç­¾å‘é‡ï¼ˆvector åŸºäº"æ ‡ç­¾æ–‡æœ¬"ï¼Œä¸å­˜å‚¨ metadataï¼‰

        Args:
            notes: ç¬”è®°æ•°æ®åˆ—è¡¨
            update_bm25: æ˜¯å¦ç«‹å³æ›´æ–°BM25ç´¢å¼•ï¼ˆé»˜è®¤Falseï¼Œå»¶è¿Ÿæ›´æ–°ä»¥æå‡æ€§èƒ½ï¼‰

        Returns:
            è®¡æ—¶å­—å…¸
        """
        import time
        timings = {}
        t_method_start = time.time()

        try:
            if not notes:
                self.logger.debug("æ²¡æœ‰ç¬”è®°éœ€è¦å­˜å‚¨")
                return timings

            # === æ‰¹é‡å¤„ç†ä¸»é›†åˆ ===
            t_prep_main = time.time()

            # å‡†å¤‡ä¸»é›†åˆæ•°æ®
            ids = [note.id for note in notes]

            # å‡†å¤‡ embedding_textsï¼ˆå†…å®¹ + æ ‡ç­¾æ–‡æœ¬ï¼‰
            embedding_texts = []
            for note in notes:
                tag_names = self.id_service.ids_to_tags(note.tag_ids)
                embedding_texts.append(note.get_embedding_text(tag_names))

            # å‡†å¤‡ metadatasï¼ˆåŒ…å«æ‰€æœ‰ç¬”è®°æ•°æ®ï¼‰
            metadatas = [note.to_dict() for note in notes]

            timings['prep_main'] = (time.time() - t_prep_main) * 1000

            # æ‰¹é‡å­˜å‚¨åˆ°ä¸»é›†åˆ
            t_main = time.time()
            upsert_timings = self.vector_store.upsert_documents(
                collection=self.main_collection,
                ids=ids,
                embedding_texts=embedding_texts,
                metadatas=metadatas,
                _return_timings=True
            )
            timings['store_main'] = (time.time() - t_main) * 1000
            if upsert_timings:
                timings['main_embed'] = upsert_timings.get('embed', 0)
                timings['main_db'] = upsert_timings.get('db_upsert', 0)

            # å¯é€‰ï¼šæ‰¹é‡æ›´æ–°BM25ç´¢å¼•
            if update_bm25 and self.vector_store._is_hybrid_search_enabled():
                collection_name = self.main_collection.name
                doc_ids = [note.id for note in notes]
                contents = [note.content for note in notes]

                success = self.vector_store.bm25_retriever.add_documents(
                    collection_name, doc_ids, contents
                )
                if success:
                    self.logger.debug(f"ğŸ“ BM25ç´¢å¼•æ‰¹é‡æ›´æ–°å®Œæˆ: {len(notes)} ä¸ªæ–‡æ¡£")
                else:
                    self.logger.warning("BM25ç´¢å¼•æ‰¹é‡æ›´æ–°å¤±è´¥")

            # === æ‰¹é‡å¤„ç†å‰¯é›†åˆ ===
            t_prep_sub = time.time()

            # å‡†å¤‡å‰¯é›†åˆæ•°æ®ï¼ˆä»…æ ‡ç­¾æ–‡æœ¬ï¼‰
            sub_ids = []
            sub_tags_texts = []
            for note in notes:
                tag_names = self.id_service.ids_to_tags(note.tag_ids)
                tags_text = note.get_tags_text(tag_names)
                sub_ids.append(note.id)
                sub_tags_texts.append(tags_text)

            timings['prep_sub'] = (time.time() - t_prep_sub) * 1000

            # æ‰¹é‡å­˜å‚¨åˆ°å‰¯é›†åˆï¼ˆä¸ä¼  metadatasï¼‰
            t_sub = time.time()
            sub_upsert_timings = self.vector_store.upsert_documents(
                collection=self.sub_collection,
                ids=sub_ids,
                embedding_texts=sub_tags_texts,
                metadatas=None,  # å‰¯é›†åˆä¸å­˜å‚¨ metadata
                _return_timings=True
            )
            timings['store_sub'] = (time.time() - t_sub) * 1000
            if sub_upsert_timings:
                timings['sub_embed'] = sub_upsert_timings.get('embed', 0)
                timings['sub_db'] = sub_upsert_timings.get('db_upsert', 0)

            # è®°å½•æ–¹æ³•æ€»ä½“æ‰§è¡Œæ—¶é—´
            timings['_batch_method_total'] = (time.time() - t_method_start) * 1000

            return timings

        except Exception as e:
            self.logger.error(f"æ‰¹é‡å­˜å‚¨æ–‡æ¡£å—å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise

    def remove_file_data(self, file_path: str) -> bool:
        """
        åˆ é™¤ä¸æŒ‡å®šæ–‡ä»¶ç›¸å…³çš„æ‰€æœ‰æ•°æ®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # å°è¯•é€šè¿‡file_idåˆ é™¤ï¼ˆä½¿ç”¨IDæœåŠ¡ï¼‰
            # è·å–ç›¸å¯¹è·¯å¾„
            relative_path = Path(file_path).name
            file_id = self.id_service.id_to_file(relative_path)
            if file_id:
                return self.remove_file_data_by_file_id(file_id)

            # ä¸´æ—¶ï¼šä»ä½¿ç”¨æ—§çš„source_file_pathæŸ¥è¯¢ï¼ˆå‘åå…¼å®¹ï¼‰
            where_clause = {"source_file_path": file_path}

            # ä»ä¸»é›†åˆä¸­è·å–æ‰€æœ‰åŒ¹é…çš„è®°å½•
            main_results = self.main_collection.get(where=where_clause)

            if main_results and main_results["ids"]:
                # è·å–è¦åˆ é™¤çš„IDåˆ—è¡¨
                ids_to_delete = main_results["ids"]

                # ä»ä¸»é›†åˆåˆ é™¤
                self.main_collection.delete(ids=ids_to_delete)

                # ä»å‰¯é›†åˆåˆ é™¤
                self.sub_collection.delete(ids=ids_to_delete)

                self.logger.info(
                    f"æˆåŠŸåˆ é™¤æ–‡ä»¶ç›¸å…³æ•°æ®: {file_path}, å…±åˆ é™¤ {len(ids_to_delete)} æ¡è®°å½•"
                )
                return True
            else:
                self.logger.info(f"æœªæ‰¾åˆ°ä¸æ–‡ä»¶ç›¸å…³çš„æ•°æ®: {file_path}")
                return True

        except Exception as e:
            self.logger.error(f"åˆ é™¤æ–‡ä»¶ç›¸å…³æ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False

    def remove_file_data_by_file_id(self, file_id: int) -> bool:
        """
        æ ¹æ®file_idåˆ é™¤æ–‡ä»¶ç›¸å…³çš„æ‰€æœ‰æ•°æ®

        Args:
            file_id: æ–‡ä»¶ID

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # æŸ¥è¯¢æ‰€æœ‰ä¸è¯¥file_idç›¸å…³çš„ç¬”è®°
            where_clause = {"file_id": file_id}
            main_results = self.main_collection.get(where=where_clause)

            if main_results and main_results["ids"]:
                # è·å–è¦åˆ é™¤çš„IDåˆ—è¡¨
                ids_to_delete = main_results["ids"]

                # ä»ä¸»é›†åˆåˆ é™¤
                self.main_collection.delete(ids=ids_to_delete)

                # ä»å‰¯é›†åˆåˆ é™¤
                self.sub_collection.delete(ids=ids_to_delete)

                self.logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ID {file_id} çš„ {len(ids_to_delete)} æ¡ç¬”è®°è®°å½•")
                return True
            else:
                self.logger.debug(f"æ–‡ä»¶ID {file_id} æ²¡æœ‰å…³è”çš„ç¬”è®°æ•°æ®")
                return True

        except Exception as e:
            self.logger.error(f"æ ¹æ®file_idåˆ é™¤æ–‡ä»¶æ•°æ®å¤±è´¥: {file_id}, é”™è¯¯: {e}")
            return False

    def close(self):
        """å…³é—­æœåŠ¡ï¼Œé‡Šæ”¾èµ„æº"""
        try:
            # å…³é—­çº¿ç¨‹æ± 
            if hasattr(self, '_thread_pool'):
                self._thread_pool.shutdown(wait=True)
                self.logger.debug("çº¿ç¨‹æ± å·²å…³é—­")

            # å…³é—­IDæœåŠ¡
            if hasattr(self, 'id_service'):
                self.id_service.close()

            self.logger.debug("ç¬”è®°æœåŠ¡å·²å…³é—­")
        except Exception as e:
            self.logger.error(f"å…³é—­ç¬”è®°æœåŠ¡å¤±è´¥: {e}")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    @classmethod
    def from_plugin_context(cls, plugin_context):
        """
        ä»PluginContextåˆ›å»ºNoteServiceå®ä¾‹

        Args:
            plugin_context: PluginContextæ’ä»¶ä¸Šä¸‹æ–‡

        Returns:
            NoteServiceå®ä¾‹
        """
        return cls(plugin_context=plugin_context)

    def get_status(self):
        """
        è·å–æœåŠ¡çŠ¶æ€

        Returns:
            åŒ…å«çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
        """
        return {
            "ready": self.collections_initialized,
            "has_vector_store": self.vector_store is not None,
            "has_plugin_context": self.plugin_context is not None,
            "provider_id": self.id_service.provider_id if hasattr(self, 'id_service') else None,
            "batch_queue_size": len(self._embedding_queue) if hasattr(self, '_embedding_queue') else 0
        }
