"""
ç¬”è®°æœåŠ¡ - LLMçš„ç¬”è®°ç®¡ç†æœåŠ¡

æä¾›ç¬”è®°çš„å¢åˆ æ”¹æŸ¥åŠŸèƒ½ï¼Œä¸è®°å¿†ç³»ç»Ÿå…±äº«åŒä¸€ä¸ªå‘é‡æ•°æ®åº“å®ä¾‹ã€‚
æ”¯æŒç›®å½•è§£æã€æ–‡æ¡£å‘é‡åŒ–å­˜å‚¨å’Œæ™ºèƒ½æŸ¥è¯¢ã€‚
"""

import re
import asyncio
import concurrent.futures
from typing import List, Dict, Optional
from pathlib import Path

from ..models.note_models import NoteData
from ..parser.parser_manager import parser_manager
from .id_service import IDService

# å¯¼å…¥æ—¥å¿—è®°å½•å™¨
try:
    from astrbot.api import logger
except ImportError:
    import logging
    logger = logging.getLogger(__name__)
from ..config.system_config import system_config


class NoteServiceError(Exception):
    """ç¬”è®°æœåŠ¡å¼‚å¸¸åŸºç±»"""

    pass


class NoteNotFoundError(NoteServiceError):
    """ç¬”è®°æœªæ‰¾åˆ°å¼‚å¸¸"""

    pass


class NoteOperationError(NoteServiceError):
    """ç¬”è®°æ“ä½œå¤±è´¥å¼‚å¸¸"""

    pass


class NoteService:
    """
    ç¬”è®°æœåŠ¡ç±»

    æä¾›LLMç¬”è®°çš„å®Œæ•´ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - ç¬”è®°çš„æ·»åŠ ã€åˆ é™¤ã€ä¿®æ”¹ã€æŸ¥è¯¢
    - ç›®å½•è§£æå’Œæ–‡æ¡£å‘é‡åŒ–
    - æ™ºèƒ½æ ‡ç­¾æå–
    - è¯­ä¹‰æœç´¢
    - ä¸è®°å¿†ç³»ç»Ÿçš„æ•°æ®éš”ç¦»
    - æ‰¹é‡å‘é‡åŒ–ä¼˜åŒ–
    """

    def __init__(self, plugin_context=None, vector_store=None):
        """
        åˆå§‹åŒ–ç¬”è®°æœåŠ¡ã€‚

        Args:
            plugin_context: PluginContextæ’ä»¶ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            vector_store: VectorStoreå®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼Œå¦‚æœæœªæä¾›plugin_contextåˆ™å¿…éœ€ï¼‰
        """
        self.logger = logger
        self.plugin_context = plugin_context

        # ä¼˜å…ˆä½¿ç”¨PluginContextï¼Œå¦åˆ™ä½¿ç”¨ä¼ å…¥çš„vector_store
        if plugin_context:
            # ä»PluginContextåˆ›å»ºIDService
            self.id_service = IDService.from_plugin_context(plugin_context)
            # vector_storeéœ€è¦åœ¨ComponentFactoryä¸­è®¾ç½®
            self.vector_store = None
            self.collections_initialized = False
            self.logger.info("ç¬”è®°æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆPluginContextæ¨¡å¼ï¼‰ï¼Œç­‰å¾…vector_storeè®¾ç½®ã€‚")
        elif vector_store:
            # å‘åå…¼å®¹æ¨¡å¼
            if not vector_store:
                raise ValueError("å¿…é¡»æä¾›ä¸€ä¸ª VectorStore å®ä¾‹ã€‚")
            self.vector_store = vector_store

            # åˆå§‹åŒ–IDæœåŠ¡ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            self.id_service = IDService()

            # é€šè¿‡VectorStoreè·å–é›†åˆ
            self._initialize_collections()
            self.logger.info("ç¬”è®°æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰ï¼Œå·²å»ºç«‹ä¸“ç”¨çš„ä¸»å‰¯é›†åˆã€‚")
        else:
            raise ValueError("å¿…é¡»æä¾› plugin_context æˆ– vector_store å‚æ•°")

        # è·å–è§£æå™¨ç®¡ç†å™¨
        self.parser_manager = parser_manager

        # åˆ›å»ºè½»é‡çº¿ç¨‹æ± ï¼ˆä»…ç”¨äºåŒæ­¥è§£æå™¨çš„å…¼å®¹ï¼Œé¡ºåºå¤„ç†æ¨¡å¼ä¸‹å‡ ä¹ä¸å¹¶å‘ï¼‰
        self._thread_pool = concurrent.futures.ThreadPoolExecutor(
            max_workers=1,  # é¡ºåºå¤„ç†ï¼Œåªéœ€1ä¸ªçº¿ç¨‹
            thread_name_prefix="NoteService"
        )

        # æ‰¹é‡å‘é‡åŒ–ä¼˜åŒ–é…ç½®
        self._batch_size = 64  # æ¯æ‰¹æ¬¡æœ€å¤š64ä¸ªæ–‡æ¡£å—
        self._batch_timeout = 5.0  # æœ€å¤§ç­‰å¾…5ç§’
        self._embedding_queue = []  # å¾…å‘é‡åŒ–çš„é˜Ÿåˆ—
        self._batch_lock = asyncio.Lock()  # å¼‚æ­¥é”ä¿æŠ¤é˜Ÿåˆ—
        self.logger.info("NoteServiceåˆå§‹åŒ–å®Œæˆï¼ˆé¡ºåºå¤„ç†æ¨¡å¼ï¼‰")

    def _initialize_collections(self):
        """åˆå§‹åŒ–ChromaDBé›†åˆ"""
        if not self.vector_store:
            raise ValueError("VectorStoreæœªè®¾ç½®ï¼Œæ— æ³•åˆå§‹åŒ–é›†åˆ")

        # é€šè¿‡VectorStoreè·å–é›†åˆï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶é¿å…é‡å¤åˆå§‹åŒ–
        self.main_collection = (
            self.vector_store.get_or_create_collection_with_dimension_check(
                system_config.notes_main_collection_name
            )
        )
        self.sub_collection = (
            self.vector_store.get_or_create_collection_with_dimension_check(
                system_config.notes_sub_collection_name
            )
        )
        self.collections_initialized = True

    def set_vector_store(self, vector_store):
        """
        è®¾ç½®VectorStoreå®ä¾‹ï¼ˆç”¨äºPluginContextæ¨¡å¼ï¼‰

        Args:
            vector_store: VectorStoreå®ä¾‹
        """
        if self.vector_store:
            self.logger.warning("VectorStoreå·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–")

        self.vector_store = vector_store
        self._initialize_collections()
        self.logger.info("VectorStoreå·²è®¾ç½®ï¼Œé›†åˆåˆå§‹åŒ–å®Œæˆ")

    def ensure_ready(self):
        """ç¡®ä¿æœåŠ¡å·²å‡†å¤‡å°±ç»ªï¼ˆç”¨äºPluginContextæ¨¡å¼ï¼‰"""
        if not self.collections_initialized:
            if not self.vector_store:
                raise RuntimeError("NoteServiceæœªè®¾ç½®VectorStoreï¼Œè¯·å…ˆè°ƒç”¨set_vector_store()")
            self._initialize_collections()
        return True

    def add_note(
        self, content: str, tags: List[str] = None, metadata: dict = None, note_id: Optional[str] = None
    ) -> str:
        """
        æ·»åŠ ç¬”è®°

        Args:
            content: ç¬”è®°å†…å®¹
            tags: æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨æå–ï¼‰
            metadata: é¢å¤–å…ƒæ•°æ®
            note_id: ç¬”è®°IDï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•æˆ–æ•°æ®è¿ç§»ï¼‰

        Returns:
            ç¬”è®°ID
        """
        try:
            # ç¡®ä¿æœåŠ¡å·²å‡†å¤‡å°±ç»ª
            self.ensure_ready()
            # è‡ªåŠ¨æå–æ ‡ç­¾ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
            if tags is None:
                tags = self._extract_tags(content)

            # è¾“å…¥éªŒè¯
            if not tags:
                raise ValueError("æ ‡ç­¾åˆ—è¡¨ä¸å…è®¸ä¸ºç©ºã€‚")

            # ä½¿ç”¨IDæœåŠ¡å°†tagå­—ç¬¦ä¸²è½¬æ¢ä¸ºtag_ids
            tag_ids = self.id_service.tags_to_ids(tags)

            # åˆ›å»ºNoteDataå¯¹è±¡
            note = NoteData.create_user_note(
                content=content,
                tag_ids=tag_ids
            )

            # å¦‚æœæä¾›äº†è‡ªå®šä¹‰IDï¼Œè¦†ç›–ç”Ÿæˆçš„ID
            if note_id is not None:
                note.id = note_id

            # ä½¿ç”¨æ–°çš„ç¬”è®°å­˜å‚¨æ–¹æ³•
            try:
                # ä½¿ç”¨VectorStoreçš„ç¬”è®°ä¸“ç”¨æ–¹æ³•å­˜å‚¨ä¸»é›†åˆ
                self.vector_store.store_note(
                    collection=self.main_collection,
                    note=note
                )

                # å‰¯é›†åˆåªå­˜å‚¨æ ‡ç­¾æ–‡æœ¬ï¼Œç”¨äºæ ‡ç­¾é‡æ’
                tag_names = self.id_service.ids_to_tags(tag_ids)
                self.vector_store.upsert_documents(
                    collection=self.sub_collection,
                    ids=[note.id],
                    embedding_texts=[note.get_tags_text(tag_names)],
                    documents=[note.get_tags_text(tag_names)]
                )

            except Exception as e:
                self.logger.error(f"å†™å…¥åŒé›†åˆå¤±è´¥ï¼Œæ‰§è¡Œå›æ»š: {e}")
                # å°è¯•å›æ»šï¼ˆåˆ é™¤å¯èƒ½å·²å†™å…¥çš„éƒ¨åˆ†æ•°æ®ï¼‰
                try:
                    self.main_collection.delete(ids=[note.id])
                    self.sub_collection.delete(ids=[note.id])
                except Exception as rollback_error:
                    self.logger.error(f"å›æ»šå¤±è´¥: {rollback_error}")
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚æ„ŸçŸ¥åˆ°å¤±è´¥

            self.logger.info(f"æˆåŠŸæ·»åŠ ç¬”è®°: {note.id}, æ ‡ç­¾: {tags}")
            return note.id

        except Exception as e:
            self.logger.error(f"æ·»åŠ ç¬”è®°å¤±è´¥: {e}")
            raise

    def search_notes(
        self, query: str, max_results: int = 10, tag_filter: List[str] = None, threshold: float = 0.5
    ) -> List[Dict]:
        """
        æœç´¢ç¬”è®°

        Args:
            query: æŸ¥è¯¢å†…å®¹
            max_results: æœ€å¤§ç»“æœæ•°
            tag_filter: æ ‡ç­¾è¿‡æ»¤
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰ï¼Œä½äºæ­¤é˜ˆå€¼çš„ç»“æœå°†è¢«è¿‡æ»¤

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ç¡®ä¿æœåŠ¡å·²å‡†å¤‡å°±ç»ª
            self.ensure_ready()
            # ä½¿ç”¨ä¸¤é˜¶æ®µæ··åˆæ£€ç´¢ç­–ç•¥ï¼Œä¼ é€’é˜ˆå€¼å‚æ•°
            results = self._hybrid_search(query, max_results, threshold=threshold)
            return results

        except Exception as e:
            self.logger.error(f"æœç´¢ç¬”è®°å¤±è´¥: {e}")
            return []

    def search_notes_by_token_limit(
        self, query: str, max_tokens: int = 10000, recall_count: int = 100, tag_filter: List[str] = None
    ) -> List[Dict]:
        """
        åŸºäºtokenæ•°é‡é™åˆ¶æœç´¢ç¬”è®°

        Args:
            query: æŸ¥è¯¢å†…å®¹
            max_tokens: æœ€å¤§tokenæ•°é™åˆ¶
            recall_count: å€™é€‰ç»“æœæ•°é‡
            tag_filter: æ ‡ç­¾è¿‡æ»¤

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼ˆä¿æŒç›¸å…³æ€§æ’åºï¼ŒæŒ‰tokené™åˆ¶åŠ¨æ€æˆªå–ï¼‰
        """
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ··åˆæ£€ç´¢ç®—æ³•è·å–é«˜è´¨é‡æ’åºç»“æœ
            candidates = self._hybrid_search(query, recall_count=recall_count, max_results=recall_count)

            # å¦‚æœå€™é€‰ç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å›
            if not candidates:
                return []

            # æŒ‰tokené™åˆ¶åŠ¨æ€é€‰æ‹©ç»“æœ
            from ..utils.token_utils import count_tokens

            selected_notes = []
            current_tokens = 0
            candidate_count = 0

            for note in candidates:
                note_content = note.get('content', '')
                note_tokens = count_tokens(note_content)

                # æ£€æŸ¥æ˜¯å¦è¶…å‡ºtokené™åˆ¶
                if current_tokens + note_tokens <= max_tokens:
                    selected_notes.append(note)
                    current_tokens += note_tokens
                    candidate_count += 1
                else:
                    # åŠ ä¸Šè¿™ä¸ªç¬”è®°ä¼šè¶…å‡ºtokené™åˆ¶ï¼Œåœæ­¢æ·»åŠ 
                    self.logger.debug(
                        f"Tokené™åˆ¶({max_tokens})è¾¾åˆ°ï¼Œé€‰æ‹©{candidate_count}ä¸ªç¬”è®°ï¼Œå…±{current_tokens} tokens"
                    )
                    break

            self.logger.debug(
                f"åŸºäºtokené™åˆ¶æœç´¢å®Œæˆ: è¿”å›{len(selected_notes)}ä¸ªç¬”è®°ï¼Œå…±{current_tokens} tokensï¼Œä»{len(candidates)}ä¸ªå€™é€‰ä¸­é€‰å‡º"
            )
            return selected_notes

        except Exception as e:
            self.logger.error(f"åŸºäºtokené™åˆ¶æœç´¢ç¬”è®°å¤±è´¥: {e}")
            return []

    def _hybrid_search(
        self,
        query: str,
        max_results: int = 10,
        recall_count: int = 100,
        threshold: float = 0.5,
    ) -> List[Dict]:
        """
        ä¸¤é˜¶æ®µæ··åˆæ£€ç´¢: å…ˆè¿‡æ»¤ï¼Œåé‡æ’ã€‚

        Args:
            query: æŸ¥è¯¢å†…å®¹
            max_results: æœ€å¤§ç»“æœæ•°
            recall_count: ç¬¬ä¸€é˜¶æ®µå¬å›æ•°é‡
            threshold: å†…å®¹ç›¸ä¼¼åº¦è¿‡æ»¤é˜ˆå€¼

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """

        # 1. ç¬¬ä¸€é˜¶æ®µï¼šè¿‡æ»¤ (Filtering)
        # ä½¿ç”¨ VectorStore çš„ç¬”è®°ä¸“ç”¨æ£€ç´¢æ–¹æ³•è¿›è¡Œå‘é‡æœç´¢
        recall_results = self.vector_store.search_notes(
            collection=self.main_collection,
            query=query,
            limit=recall_count
        )

        # å¤„ç†æ— ç»“æœæƒ…å†µ
        if not recall_results:
            return []

        # æ„å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å¬å›ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œå¹¶åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        all_recalled_notes = []
        for note in recall_results:
            # è·å–çœŸå®çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç”±VectorStoreè®¾ç½®ï¼‰
            content_similarity = getattr(note, 'similarity', 0.0)

            # åº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼šåªä¿ç•™ç›¸ä¼¼åº¦ >= threshold çš„ç»“æœ
            if content_similarity < threshold:
                continue

            # ä» NoteData å¯¹è±¡ä¸­æå–æ•°æ®
            tag_names = self.id_service.ids_to_tags(note.tag_ids)
            all_recalled_notes.append(
                {
                    "id": note.id,
                    "content": note.content,
                    "metadata": note.to_dict(),
                    "tags": tag_names,  # è½¬æ¢ä¸ºtag_names
                    "content_similarity": content_similarity,  # ä½¿ç”¨çœŸå®ç›¸ä¼¼åº¦
                }
            )

        # å¦‚æœæ²¡æœ‰ç¬”è®°é€šè¿‡é˜ˆå€¼è¿‡æ»¤ï¼Œç›´æ¥è¿”å›
        if not all_recalled_notes:
            self.logger.debug(f"æ‰€æœ‰å¬å›ç»“æœéƒ½ä½äºé˜ˆå€¼ {threshold}ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        # 2. ç¬¬ä¸€é˜¶æ®µï¼šé‡æ’ (Reranking)
        # æ‹¿ç€åŸå§‹æŸ¥è¯¢ï¼Œåœ¨æ•´ä¸ªå‰¯é›†åˆ (çº¯æ ‡ç­¾) ä¸­è¿›è¡Œæœç´¢ï¼Œä»¥è·å–æ‰€æœ‰ç¬”è®°çš„æ ‡ç­¾ç›¸å…³æ€§åˆ†æ•°
        all_sub_collection_ids = [note["id"] for note in all_recalled_notes]

        # ä½¿ç”¨ VectorStore çš„ç¬”è®°ä¸“ç”¨æ£€ç´¢æ–¹æ³•è¿›è¡Œæ ‡ç­¾é‡æ’
        rerank_results = self.vector_store.search_notes(
            collection=self.sub_collection,
            query=query,
            limit=len(all_sub_collection_ids)
        )

        # åˆ›å»ºä¸€ä¸ª "ID -> æ ‡ç­¾åˆ†æ•°" çš„æ˜ å°„ï¼Œä½¿ç”¨çœŸå®çš„ç›¸ä¼¼åº¦åˆ†æ•°
        tag_scores = {}
        for note in (rerank_results or []):
            if note.id in all_sub_collection_ids:
                # ä½¿ç”¨çœŸå®çš„æ ‡ç­¾ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç”±VectorStoreè®¾ç½®ï¼‰
                tag_similarity = getattr(note, 'similarity', 0.0)
                tag_scores[note.id] = tag_similarity

        # 3. æœ€ç»ˆæ’åº
        # å°†æ ‡ç­¾åˆ†æ•°é™„åŠ åˆ°é€šè¿‡ç¬¬ä¸€é˜¶æ®µçš„ç¬”è®°ä¸Š
        for note in all_recalled_notes:
            note["tag_score"] = tag_scores.get(
                note["id"], 0.0
            )  # å¦‚æœåœ¨å‰¯é›†åˆä¸­æ²¡æ‰¾åˆ°ï¼Œåˆ™æ ‡ç­¾åˆ†ä¸º0

        # æ ¹æ®æ ‡ç­¾åˆ†æ•°è¿›è¡Œé™åºæ’åº
        all_recalled_notes.sort(key=lambda x: x["tag_score"], reverse=True)

        # æ–‡æ¡£å»é‡ï¼šåœ¨åŒä¸€æ–‡æ¡£ä¸­åªä¿ç•™æ’åæœ€é«˜çš„ç‰‡æ®µ
        # ç”±äºå·²æŒ‰æ ‡ç­¾åˆ†æ•°æ’åºï¼Œç¬¬ä¸€æ¬¡å‡ºç°çš„å°±æ˜¯è¯¥æ–‡æ¡£ä¸­æ ‡ç­¾æœ€ç›¸å…³çš„å—
        seen_file_ids = set()
        deduplicated_notes = []

        for note in all_recalled_notes:
            file_id = note["metadata"].get("file_id")
            if file_id is None:
                self.logger.warning(f"Note {note['id']} is missing 'file_id' in metadata, skipping deduplication for this item.")
                deduplicated_notes.append(note)
                continue

            if file_id not in seen_file_ids:
                seen_file_ids.add(file_id)
                deduplicated_notes.append(note)

        # 4. ç»„è£…æœ€ç»ˆç»“æœ
        # æˆªå–æ‰€éœ€æ•°é‡çš„ç»“æœï¼Œå¹¶æ ¼å¼åŒ–è¾“å‡º
        final_results = []
        for note in deduplicated_notes[:max_results]:
            final_results.append(
                {
                    "id": note["id"],
                    "content": note["content"],
                    "metadata": note["metadata"],
                    "tags": note["tags"],
                    "similarity": note[
                        "content_similarity"
                    ],  # è¿”å›å†…å®¹ç›¸ä¼¼åº¦ï¼Œå› ä¸ºè¿™æ›´ç›´è§‚
                }
            )

        return final_results

    def get_note(self, note_id: str) -> Dict:
        """
        è·å–æŒ‡å®šç¬”è®°

        Args:
            note_id: ç¬”è®°ID

        Returns:
            ç¬”è®°å†…å®¹å­—å…¸

        Raises:
            NoteNotFoundError: å½“ç¬”è®°ä¸å­˜åœ¨æ—¶
            NoteOperationError: å½“è·å–è¿‡ç¨‹ä¸­å‘ç”Ÿå…¶ä»–é”™è¯¯æ—¶
        """
        try:
            # ä¸»è¦ä¿¡æ¯ä»ä¸»é›†åˆè·å–
            results = self.main_collection.get(ids=[note_id])

            if results and results["ids"] and results["ids"][0]:
                # ç»„è£…è¿”å›æ•°æ®
                metadata = results["metadatas"][0] if results["metadatas"] else {}
                document = metadata.get("content", "")

                # ä»tag_idsè½¬æ¢ä¸ºtag_names
                tag_ids = metadata.get("tag_ids", [])
                if isinstance(tag_ids, str):
                    import json
                    try:
                        tag_ids = json.loads(tag_ids)
                    except json.JSONDecodeError:
                        tag_ids = []

                # åˆ›å»ºNoteDataå¯¹è±¡ä»¥è·å–tag_names
                note_data = NoteData.from_dict(metadata)
                tag_names = self.id_service.ids_to_tags(note_data.tag_ids)

                formatted_note = {
                    "id": note_id,
                    "content": document,
                    "tags": tag_names,
                    "metadata": metadata,
                }

                return formatted_note
            else:
                self.logger.warning(f"ç¬”è®°ä¸å­˜åœ¨: {note_id}")
                raise NoteNotFoundError(f"ç¬”è®°ä¸å­˜åœ¨: {note_id}")

        except NoteServiceError:
            # é‡æ–°æŠ›å‡ºæˆ‘ä»¬è‡ªå·±çš„å¼‚å¸¸
            raise
        except Exception as e:
            self.logger.error(f"è·å–ç¬”è®°å¤±è´¥: {e}")
            raise NoteOperationError(f"è·å–ç¬”è®°å¤±è´¥: {e}") from e

    def delete_note(self, note_id: str) -> bool:
        """
        åˆ é™¤ç¬”è®°

        Args:
            note_id: ç¬”è®°ID

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # é¦–å…ˆæ£€æŸ¥ç¬”è®°æ˜¯å¦å­˜åœ¨ï¼Œä»¥æä¾›æ›´å¥½çš„é”™è¯¯ä¿¡æ¯
            try:
                self.get_note(note_id)
            except NoteNotFoundError:
                self.logger.warning(f"è¦åˆ é™¤çš„ç¬”è®°ä¸å­˜åœ¨: {note_id}")
                return False

            # äº‹åŠ¡æ€§åˆ é™¤ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            try:
                self.main_collection.delete(ids=[note_id])
                self.sub_collection.delete(ids=[note_id])
                self.logger.info(f"æˆåŠŸåˆ é™¤ç¬”è®°: {note_id}")
                return True
            except Exception as e:
                self.logger.error(f"åˆ é™¤åŒé›†åˆä¸­çš„ {note_id} å¤±è´¥: {e}")
                # æ³¨æ„ï¼šè¿™é‡Œçš„å›æ»šæ¯”è¾ƒå›°éš¾ï¼Œä½†è‡³å°‘è®°å½•äº†é”™è¯¯
                return False

        except NoteServiceError:
            # ç¬”è®°ä¸å­˜åœ¨å·²ç»è¢«ä¸Šé¢å¤„ç†ï¼Œè¿™é‡Œå¤„ç†å…¶ä»–é”™è¯¯
            return False
        except Exception as e:
            self.logger.error(f"åˆ é™¤ç¬”è®°å¤±è´¥: {e}")
            return False

    def _extract_tags(self, content: str) -> List[str]:
        """
        ä»å†…å®¹ä¸­è‡ªåŠ¨æå–æ ‡ç­¾

        Args:
            content: æ–‡æœ¬å†…å®¹

        Returns:
            æå–çš„æ ‡ç­¾åˆ—è¡¨
        """
        tags = []

        # 1. æå–æ ‡é¢˜ï¼ˆ# ## ###ï¼‰
        title_pattern = r"^#{1,6}\s+(.+)$"
        titles = re.findall(title_pattern, content, re.MULTILINE)
        tags.extend(titles)

        # 2. æå–åŠ ç²—æ–‡æœ¬ï¼ˆ**text**ï¼‰
        bold_pattern = r"\*\*([^*]+)\*\*"
        bold_texts = re.findall(bold_pattern, content)
        tags.extend(bold_texts)

        # 3. æå–å¼•å·æ–‡æœ¬
        quote_patterns = [
            r'"([^"]+)"',  # æå–åŒå¼•å·å†…çš„æ–‡æœ¬
            r"'([^']+)'",  # æå–å•å¼•å·å†…çš„æ–‡æœ¬
        ]

        for pattern in quote_patterns:
            quotes = re.findall(pattern, content)
            tags.extend(quotes)

        # 4. æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤æ‚çš„å…³é”®è¯æå–é€»è¾‘

        # å»é‡å’Œæ¸…ç†
        tags = list(set(tags))  # å»é‡
        tags = [tag.strip() for tag in tags if tag.strip()]  # å»ç©ºæ ¼
        tags = [tag for tag in tags if len(tag) > 0]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼Œä½†ä¿ç•™å•å­—ç¬¦æ ‡ç­¾

        return tags

    def update_note(
        self, note_id: str, content: str = None, tags: List[str] = None
    ) -> None:
        """
        æ›´æ–°ç¬”è®°

        Args:
            note_id: ç¬”è®°ID
            content: æ–°å†…å®¹ï¼ˆå¯é€‰ï¼‰
            tags: æ–°æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰

        Raises:
            NoteNotFoundError: å½“ç¬”è®°ä¸å­˜åœ¨æ—¶
            NoteOperationError: å½“æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿå…¶ä»–é”™è¯¯æ—¶
        """
        try:
            # è·å–ç°æœ‰ç¬”è®°ä»¥è·å–å½“å‰å†…å®¹
            old_note_dict = self.get_note(note_id)

            # å°†å­—å…¸è½¬æ¢ä¸ºNoteDataå¯¹è±¡
            old_note = NoteData.from_dict(old_note_dict["metadata"])

            # ä½¿ç”¨æ–°å†…å®¹æˆ–ä¿ç•™æ—§å†…å®¹
            final_content = content if content is not None else old_note.content

            # ä½¿ç”¨æ–°æ ‡ç­¾æˆ–é‡æ–°æå–æ ‡ç­¾
            final_tags = tags if tags is not None else self._extract_tags(final_content)

            # ä½¿ç”¨IDæœåŠ¡å°†tagå­—ç¬¦ä¸²è½¬æ¢ä¸ºtag_ids
            final_tag_ids = self.id_service.tags_to_ids(final_tags)

            # æ›´æ–°ç¬”è®°å¯¹è±¡ï¼ˆç›´æ¥ä¿®æ”¹dataclasså­—æ®µï¼‰
            old_note.content = final_content
            old_note.tag_ids = final_tag_ids

            # ä½¿ç”¨æ–°çš„ç¬”è®°å­˜å‚¨æ–¹æ³•æ›´æ–°ä¸»é›†åˆ
            try:
                self.vector_store.store_note(
                    collection=self.main_collection,
                    note=old_note
                )

                # æ›´æ–°å‰¯é›†åˆ
                tag_names = self.id_service.ids_to_tags(old_note.tag_ids)
                self.vector_store.upsert_documents(
                    collection=self.sub_collection,
                    ids=[old_note.id],
                    embedding_texts=[old_note.get_tags_text(tag_names)],
                    documents=[old_note.get_tags_text(tag_names)]
                )

            except Exception as e:
                self.logger.error(f"æ›´æ–°åŒé›†åˆå¤±è´¥: {e}")
                raise

            self.logger.info(f"æˆåŠŸæ›´æ–°ç¬”è®°: {note_id}")

        except NoteServiceError:
            # é‡æ–°æŠ›å‡ºæˆ‘ä»¬è‡ªå·±çš„å¼‚å¸¸
            raise
        except Exception as e:
            self.logger.error(f"æ›´æ–°ç¬”è®°å¤±è´¥: {e}")
            raise NoteOperationError(f"æ›´æ–°ç¬”è®°å¤±è´¥: {e}") from e

    # ===== æ–°å¢åŠŸèƒ½ï¼šç›®å½•è§£æå’Œæ–‡æ¡£å‘é‡åŒ– =====

    def parse_and_store_directory(self, directory_path: str) -> int:
        """
        è§£æç›®å½•ä¸­çš„æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ï¼Œå¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

        Args:
            directory_path: ç›®å½•è·¯å¾„

        Returns:
            å¤„ç†çš„æ–‡ä»¶æ•°é‡
        """
        try:
            directory = Path(directory_path)
            if not directory.exists():
                raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")

            if not directory.is_dir():
                raise ValueError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {directory_path}")

            processed_count = 0

            # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for file_path in directory.rglob("*"):
                if file_path.is_file():
                    try:
                        # ç”±æœåŠ¡å±‚å†³å®šæ˜¯å¦æ”¯æŒè¯¥æ–‡ä»¶ç±»å‹
                        if self._is_supported_file(str(file_path)):
                            self.parse_and_store_file(str(file_path))
                            processed_count += 1
                            self.logger.info(f"æˆåŠŸå¤„ç†æ–‡ä»¶: {file_path}")
                    except Exception as e:
                        self.logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                        continue

            self.logger.info(f"ç›®å½•è§£æå®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
            return processed_count

        except Exception as e:
            self.logger.error(f"è§£æç›®å½•å¤±è´¥: {e}")
            raise

    def parse_and_store_file(self, file_path: str) -> int:
        """
        è§£æå•ä¸ªæ–‡ä»¶å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            å¤„ç†çš„æ–‡æ¡£å—æ•°é‡
        """
        try:
            file_path_obj = Path(file_path)
            if not file_path_obj.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æ”¯æŒ
            if not self._is_supported_file(file_path):
                self.logger.debug(f"æ–‡ä»¶ç±»å‹ä¸æ”¯æŒï¼Œè·³è¿‡å¤„ç†: {file_path}")
                return 0

            # è·å–å¯¹åº”çš„è§£æå™¨ï¼ˆä¼ é€’IDæœåŠ¡ä¸­çš„TagManagerï¼‰
            parser = self.parser_manager.get_parser_for_file(file_path, self.id_service.tag_manager)
            if not parser:
                self.logger.warning(f"æœªæ‰¾åˆ°é€‚åˆçš„è§£æå™¨ï¼Œè·³è¿‡å¤„ç†: {file_path}")
                return 0

            # è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆä½†ä¸åˆ›å»ºæ–‡ä»¶ç´¢å¼•ï¼‰
            file_path_obj = Path(file_path)
            file_timestamp = int(file_path_obj.stat().st_mtime)
            relative_path = file_path_obj.name  # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºç›¸å¯¹è·¯å¾„

            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œåˆ›å»ºæ–‡ä»¶ç´¢å¼•ï¼Œç”±è°ƒç”¨æ–¹åœ¨å¤„ç†æˆåŠŸååˆ›å»º
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶IDç”¨äºè§£æè¿‡ç¨‹ä¸­çš„æ ‡è¯†
            temp_file_id = hash(relative_path + str(file_timestamp)) % (2**31)  # ç”Ÿæˆä¸´æ—¶ID

            # ä½¿ç”¨å¸¦TagManagerçš„è§£æå™¨
            if hasattr(parser, "async_parse"):
                # å¼‚æ­¥è§£æéœ€è¦ä¼ é€’TagManagerå’Œfile_id
                loop = asyncio.new_event_loop()
                document_blocks = loop.run_until_complete(parser.async_parse(file_path, self.id_service.tag_manager, temp_file_id))
            else:
                # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è§£æ
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                except UnicodeDecodeError:
                    # å¯¹äºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¼ é€’ç©ºå†…å®¹ï¼Œç”±è§£æå™¨å¤„ç†
                    content = ""

                # è§£ææ–‡æ¡£ï¼Œä¼ é€’ä¸´æ—¶file_id
                document_blocks = parser.parse(content, temp_file_id, file_path)

            # æ‰¹é‡å­˜å‚¨æ‰€æœ‰ç¬”è®°ï¼Œè€Œä¸æ˜¯é€ä¸ªå­˜å‚¨ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰
            if document_blocks:
                self._store_notes_batch(document_blocks)

            return len(document_blocks)

        except Exception as e:
            self.logger.error(f"è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            raise

    def parse_and_store_file_sync(self, file_path: str, relative_path: str = None) -> tuple:
        """
        åŒæ­¥ç‰ˆæœ¬ï¼šè§£æå¹¶å­˜å‚¨æ–‡ä»¶ï¼ˆé¡ºåºå¤„ç†ä¼˜åŒ–ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            relative_path: ç›¸å¯¹è·¯å¾„
            
        Returns:
            (æ–‡æ¡£æ•°é‡, è®¡æ—¶å­—å…¸)
        """
        import time
        timings = {}
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_path_obj = Path(file_path)
            if not file_path_obj.exists():
                self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return 0, timings
            
            # è·å–è§£æå™¨
            t_parser = time.time()
            parser = self.parser_manager.get_parser_for_file(file_path, self.id_service.tag_manager)
            timings['parser_select'] = (time.time() - t_parser) * 1000
            
            if parser is None:
                self.logger.warning(f"æœªæ‰¾åˆ°é€‚åˆçš„è§£æå™¨ï¼Œè·³è¿‡å¤„ç†: {file_path}")
                return 0, timings
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_path_obj = Path(file_path)
            file_timestamp = int(file_path_obj.stat().st_mtime)
            
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„relative_path
            if relative_path is None:
                relative_path = file_path_obj.name
            
            # è·å–æˆ–åˆ›å»ºæ–‡ä»¶ID
            t_start = time.time()
            file_id = self.id_service.file_to_id(relative_path, file_timestamp)
            timings['id_lookup'] = (time.time() - t_start) * 1000
            
            # åŒæ­¥è§£ææ–‡ä»¶
            t_start = time.time()
            document_blocks = self._parse_file_sync(file_path, parser, file_id)
            timings['parse'] = (time.time() - t_start) * 1000
            
            # å­˜å‚¨notesï¼ˆå®Œå…¨åŒæ­¥ç‰ˆæœ¬ï¼Œç›´æ¥å­˜å‚¨ä¸èµ°é˜Ÿåˆ—ï¼‰
            if document_blocks:
                t_store_submit = time.time()
                
                # ç›´æ¥åŒæ­¥å­˜å‚¨ï¼ˆè·³è¿‡æ‰¹é‡é˜Ÿåˆ—ï¼Œä¸æ›´æ–°BM25ï¼‰
                store_timings = self._store_notes_batch(document_blocks, update_bm25=False)
                
                timings['store_total'] = (time.time() - t_store_submit) * 1000
                
                if store_timings:
                    timings.update(store_timings)
            else:
                timings['store_total'] = 0
            
            return len(document_blocks), timings
            
        except Exception as e:
            self.logger.error(f"åŒæ­¥è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            raise
    
    async def async_parse_and_store_file(self, file_path: str, relative_path: str = None) -> tuple:
        """
        å¼‚æ­¥è§£æå•ä¸ªæ–‡ä»¶å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            relative_path: ç›¸å¯¹è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨æ–‡ä»¶åï¼‰

        Returns:
            (æ–‡æ¡£å—æ•°é‡, è®¡æ—¶å­—å…¸)
        """
        import time
        timings = {}
        
        try:
            file_path_obj = Path(file_path)
            if not file_path_obj.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æ”¯æŒ
            if not self._is_supported_file(file_path):
                self.logger.debug(f"æ–‡ä»¶ç±»å‹ä¸æ”¯æŒï¼Œè·³è¿‡å¤„ç†: {file_path}")
                return 0, timings

            # è·å–å¯¹åº”çš„è§£æå™¨ï¼ˆä¼ é€’IDæœåŠ¡ä¸­çš„TagManagerï¼‰
            parser = self.parser_manager.get_parser_for_file(file_path, self.id_service.tag_manager)
            if not parser:
                self.logger.warning(f"æœªæ‰¾åˆ°é€‚åˆçš„è§£æå™¨ï¼Œè·³è¿‡å¤„ç†: {file_path}")
                return 0, timings

            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_path_obj = Path(file_path)
            file_timestamp = int(file_path_obj.stat().st_mtime)

            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„relative_pathï¼Œå¦åˆ™ä½¿ç”¨æ–‡ä»¶å
            if relative_path is None:
                relative_path = file_path_obj.name

            # è·å–æˆ–åˆ›å»ºæ–‡ä»¶ID
            t_start = time.time()
            file_id = self.id_service.file_to_id(relative_path, file_timestamp)
            timings['id_lookup'] = (time.time() - t_start) * 1000

            # ä½¿ç”¨å¸¦TagManagerçš„è§£æå™¨
            t_start = time.time()
            if hasattr(parser, "async_parse"):
                # å¼‚æ­¥è§£æéœ€è¦ä¼ é€’TagManagerå’Œfile_id
                document_blocks = await parser.async_parse(file_path, self.id_service.tag_manager, file_id)
            else:
                # åœ¨å…±äº«çº¿ç¨‹æ± ä¸­æ‰§è¡ŒCPUå¯†é›†å‹ä»»åŠ¡ï¼ˆå¤ç”¨çº¿ç¨‹ï¼Œé¿å…æ— é™åˆ›å»ºï¼‰
                loop = asyncio.get_event_loop()
                document_blocks = await loop.run_in_executor(
                    self._thread_pool, self._parse_file_sync, file_path, parser, file_id
                )
            timings['parse'] = (time.time() - t_start) * 1000

            # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œclose()ï¼Œå› ä¸ºåç»­å­˜å‚¨notesè¿˜éœ€è¦åˆ›å»ºtags
            # close()åªåº”è¯¥åœ¨æ•´ä¸ªæœåŠ¡å…³é—­æ—¶è°ƒç”¨
            timings['id_close'] = 0

            # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡å­˜å‚¨ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
            if document_blocks:
                t_store_submit = time.time()
                
                # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡å­˜å‚¨æ–¹æ³•
                store_timings = await self._store_notes_batch_optimized(document_blocks)
                
                timings['store_total'] = (time.time() - t_store_submit) * 1000
                
                # åˆå¹¶storeå†…éƒ¨çš„è¯¦ç»†è®¡æ—¶
                if store_timings:
                    timings.update(store_timings)
            else:
                timings['store_total'] = 0

            return len(document_blocks), timings

        except Exception as e:
            self.logger.error(f"å¼‚æ­¥è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            raise

    async def parse_and_store_with_file_id(
        self,
        file_path: str,
        file_id: int,
        relative_path: str
    ) -> tuple:
        """
        ä½¿ç”¨é¢†å¯¼åˆ†é…çš„file_idå¤„ç†æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆï¼Œä¸æŸ¥æ•°æ®åº“ï¼‰

        Args:
            file_path: å®Œæ•´æ–‡ä»¶è·¯å¾„
            file_id: é¢†å¯¼é¢„å…ˆåˆ†é…çš„æ–‡ä»¶ID
            relative_path: ç›¸å¯¹è·¯å¾„

        Returns:
            (æ–‡æ¡£å—æ•°é‡, è®¡æ—¶å­—å…¸)

        Raises:
            Exception: å¤„ç†å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ç›´æ¥ä½¿ç”¨é¢†å¯¼åˆ†é…çš„file_idï¼Œä¸æŸ¥æ•°æ®åº“
        doc_count, timings = await self.async_parse_and_store_file(file_path, relative_path)
        return doc_count, timings

    async def parse_and_store_with_rollback(
        self,
        file_path: str,
        file_index_manager,
        relative_path: str,
        timestamp: int
    ) -> tuple:
        """
        åŸå­æ€§åœ°å¤„ç†æ–‡ä»¶ï¼šå…ˆç´¢å¼•ï¼Œå†å‘é‡ï¼Œå¤±è´¥åˆ™å›æ»šç´¢å¼•ï¼ˆæ—§ç‰ˆæœ¬ï¼Œå…¼å®¹æ€§ä¿ç•™ï¼‰

        Args:
            file_path: å®Œæ•´æ–‡ä»¶è·¯å¾„
            file_index_manager: æ–‡ä»¶ç´¢å¼•ç®¡ç†å™¨å®ä¾‹
            relative_path: ç›¸å¯¹è·¯å¾„
            timestamp: æ–‡ä»¶æ—¶é—´æˆ³

        Returns:
            (æ–‡æ¡£å—æ•°é‡, è®¡æ—¶å­—å…¸)

        Raises:
            Exception: å‘é‡åº“æ“ä½œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸ï¼Œç´¢å¼•å·²è¢«å›æ»š
        """
        # 1. å…ˆåˆ›å»ºæ–‡ä»¶ç´¢å¼•ï¼ˆç®€å•å¿«é€Ÿï¼‰
        file_id = file_index_manager.get_or_create_file_id(relative_path, timestamp)

        try:
            # 2. å¤„ç†å‘é‡åº“ï¼Œä¼ é€’æ­£ç¡®çš„relative_pathï¼Œè¿”å›æ–‡æ¡£æ•°å’Œè®¡æ—¶
            doc_count, timings = await self.async_parse_and_store_file(file_path, relative_path)
            return doc_count, timings
        except Exception as e:
            # 3. å‘é‡åº“å¤±è´¥ï¼Œå›æ»šç´¢å¼•
            self.logger.error(f"å‘é‡åº“å¤„ç†å¤±è´¥ï¼Œå›æ»šæ–‡ä»¶ç´¢å¼•: {relative_path}, é”™è¯¯: {e}")
            try:
                file_index_manager.delete_file(file_id)
                self.logger.debug(f"å·²å›æ»šæ–‡ä»¶ç´¢å¼•: {relative_path} (ID: {file_id})")
            except Exception as rollback_error:
                self.logger.error(f"å›æ»šæ–‡ä»¶ç´¢å¼•å¤±è´¥: {relative_path}, é”™è¯¯: {rollback_error}")
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨æ–¹çŸ¥é“å¤±è´¥äº†

    def _parse_file_sync(self, file_path: str, parser, file_id: int) -> List[NoteData]:
        """
        åŒæ­¥è§£ææ–‡ä»¶çš„è¾…åŠ©æ–¹æ³•

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            parser: è§£æå™¨å®ä¾‹
            file_id: æ–‡ä»¶ç´¢å¼•ID

        Returns:
            ç¬”è®°æ•°æ®åˆ—è¡¨
        """
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
            except UnicodeDecodeError:
                # å¯¹äºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¼ é€’ç©ºå†…å®¹ï¼Œç”±è§£æå™¨å¤„ç†
                content = ""

            # è§£ææ–‡æ¡£ï¼Œä¼ é€’file_idï¼ˆè§£æå™¨å·²ç»æœ‰TagManagerï¼‰
            return parser.parse(content, file_id, file_path)
        except Exception as e:
            self.logger.error(f"åŒæ­¥è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            raise

    def _is_supported_file(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒè§£æ

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦æ”¯æŒ
        """
        path = Path(file_path)
        extension = path.suffix.lower()
        return self.parser_manager.is_supported_extension(extension)

    def _store_notes_batch(self, notes: List[NoteData], update_bm25: bool = False) -> dict:
        """
        æ‰¹é‡å­˜å‚¨ç¬”è®°åˆ°å‘é‡æ•°æ®åº“ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰

        Args:
            notes: ç¬”è®°æ•°æ®åˆ—è¡¨
            update_bm25: æ˜¯å¦ç«‹å³æ›´æ–°BM25ç´¢å¼•ï¼ˆé»˜è®¤Falseï¼Œå»¶è¿Ÿæ›´æ–°ä»¥æå‡æ€§èƒ½ï¼‰
            
        Returns:
            è®¡æ—¶å­—å…¸
        """
        import time
        timings = {}
        t_method_start = time.time()  # æ–¹æ³•æ€»ä½“è®¡æ—¶
        
        try:
            if not notes:
                self.logger.debug("æ²¡æœ‰ç¬”è®°éœ€è¦å­˜å‚¨")
                return timings

            # å‡†å¤‡å‰¯é›†åˆæ•°æ®
            t_prep_sub = time.time()
            notes_to_store = notes
            sub_collection_data = []

            for note in notes:
                tag_names = self.id_service.ids_to_tags(note.tag_ids)
                sub_collection_data.append({
                    "id": note.id,
                    "tags_text": note.get_tags_text(tag_names)
                })
            timings['prep_sub'] = (time.time() - t_prep_sub) * 1000

            # === æ‰¹é‡å¤„ç†ä¸»é›†åˆï¼ˆåŒæ­¥è°ƒç”¨ï¼‰ ===
            if notes_to_store:
                # å‡†å¤‡ä¸»é›†åˆæ•°æ® - è¯¦ç»†è®¡æ—¶
                t_prep_main = time.time()
                
                ids = [note.id for note in notes_to_store]
                timings['prep_main_ids'] = (time.time() - t_prep_main) * 1000
                
                # å‡†å¤‡embedding_textsï¼ˆåŒ…å«ids_to_tagsè°ƒç”¨ï¼‰
                t_embed_texts = time.time()
                embedding_texts = []
                for note in notes_to_store:
                    tag_names = self.id_service.ids_to_tags(note.tag_ids)
                    embedding_texts.append(note.get_embedding_text(tag_names))
                timings['prep_main_embed_texts'] = (time.time() - t_embed_texts) * 1000
                
                # å‡†å¤‡documents
                t_docs = time.time()
                documents = [note.content for note in notes_to_store]
                timings['prep_main_docs'] = (time.time() - t_docs) * 1000
                
                # å‡†å¤‡metadatas
                t_meta = time.time()
                metadatas = [note.to_dict() for note in notes_to_store]
                timings['prep_main_meta'] = (time.time() - t_meta) * 1000

                # ä¸€æ¬¡æ€§æ‰¹é‡å­˜å‚¨æ‰€æœ‰æ–‡æ¡£å—ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œæ•°æ®åº“å†…éƒ¨å¤„ç†å¹¶å‘ï¼‰
                t_main = time.time()
                upsert_timings = self.vector_store.upsert_documents(
                    collection=self.main_collection,
                    ids=ids,
                    embedding_texts=embedding_texts,
                    documents=documents,
                    metadatas=metadatas,
                    _return_timings=True
                )
                timings['store_main'] = (time.time() - t_main) * 1000
                if upsert_timings:
                    timings['main_embed'] = upsert_timings.get('embed', 0)
                    timings['main_db'] = upsert_timings.get('db_upsert', 0)

                # å¯é€‰ï¼šæ‰¹é‡æ›´æ–°BM25ç´¢å¼•
                if update_bm25 and self.vector_store._is_hybrid_search_enabled():
                    collection_name = self.main_collection.name
                    doc_ids = [note.id for note in notes_to_store]
                    contents = [note.content for note in notes_to_store]

                    success = self.vector_store.bm25_retriever.add_documents(
                        collection_name, doc_ids, contents
                    )
                    if success:
                        self.logger.debug(f"ğŸ“ BM25ç´¢å¼•æ‰¹é‡æ›´æ–°å®Œæˆ: {len(notes_to_store)} ä¸ªæ–‡æ¡£")
                    else:
                        self.logger.warning("BM25ç´¢å¼•æ‰¹é‡æ›´æ–°å¤±è´¥")

            # === æ‰¹é‡å¤„ç†å‰¯é›†åˆï¼ˆåŒæ­¥è°ƒç”¨ï¼‰ ===
            if sub_collection_data:
                t_sub = time.time()
                ids = [data["id"] for data in sub_collection_data]
                tags_texts = [data["tags_text"] for data in sub_collection_data]
                sub_upsert_timings = self.vector_store.upsert_documents(
                    collection=self.sub_collection,
                    ids=ids,
                    embedding_texts=tags_texts,
                    documents=tags_texts,
                    _return_timings=True
                )
                timings['store_sub'] = (time.time() - t_sub) * 1000
                if sub_upsert_timings:
                    timings['sub_embed'] = sub_upsert_timings.get('embed', 0)
                    timings['sub_db'] = sub_upsert_timings.get('db_upsert', 0)
            
            # è®°å½•æ–¹æ³•æ€»ä½“æ‰§è¡Œæ—¶é—´
            timings['_batch_method_total'] = (time.time() - t_method_start) * 1000
            
            return timings

        except Exception as e:
            self.logger.error(f"æ‰¹é‡å­˜å‚¨æ–‡æ¡£å—å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise

    def remove_file_data(self, file_path: str) -> bool:
        """
        åˆ é™¤ä¸æŒ‡å®šæ–‡ä»¶ç›¸å…³çš„æ‰€æœ‰æ•°æ®ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # å°è¯•é€šè¿‡file_idåˆ é™¤ï¼ˆä½¿ç”¨IDæœåŠ¡ï¼‰
            # è·å–ç›¸å¯¹è·¯å¾„
            relative_path = Path(file_path).name
            file_id = self.id_service.id_to_file(relative_path)
            if file_id:
                return self.remove_file_data_by_file_id(file_id)

            # ä¸´æ—¶ï¼šä»ä½¿ç”¨æ—§çš„source_file_pathæŸ¥è¯¢ï¼ˆå‘åå…¼å®¹ï¼‰
            where_clause = {"source_file_path": file_path}

            # ä»ä¸»é›†åˆä¸­è·å–æ‰€æœ‰åŒ¹é…çš„è®°å½•
            main_results = self.main_collection.get(where=where_clause)

            if main_results and main_results["ids"]:
                # è·å–è¦åˆ é™¤çš„IDåˆ—è¡¨
                ids_to_delete = main_results["ids"]

                # ä»ä¸»é›†åˆåˆ é™¤
                self.main_collection.delete(ids=ids_to_delete)

                # ä»å‰¯é›†åˆåˆ é™¤
                self.sub_collection.delete(ids=ids_to_delete)

                self.logger.info(
                    f"æˆåŠŸåˆ é™¤æ–‡ä»¶ç›¸å…³æ•°æ®: {file_path}, å…±åˆ é™¤ {len(ids_to_delete)} æ¡è®°å½•"
                )
                return True
            else:
                self.logger.info(f"æœªæ‰¾åˆ°ä¸æ–‡ä»¶ç›¸å…³çš„æ•°æ®: {file_path}")
                return True

        except Exception as e:
            self.logger.error(f"åˆ é™¤æ–‡ä»¶ç›¸å…³æ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False

    def remove_file_data_by_file_id(self, file_id: int) -> bool:
        """
        æ ¹æ®file_idåˆ é™¤æ–‡ä»¶ç›¸å…³çš„æ‰€æœ‰æ•°æ®

        Args:
            file_id: æ–‡ä»¶ID

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # æŸ¥è¯¢æ‰€æœ‰ä¸è¯¥file_idç›¸å…³çš„ç¬”è®°
            where_clause = {"file_id": file_id}
            main_results = self.main_collection.get(where=where_clause)

            if main_results and main_results["ids"]:
                # è·å–è¦åˆ é™¤çš„IDåˆ—è¡¨
                ids_to_delete = main_results["ids"]

                # ä»ä¸»é›†åˆåˆ é™¤
                self.main_collection.delete(ids=ids_to_delete)

                # ä»å‰¯é›†åˆåˆ é™¤
                self.sub_collection.delete(ids=ids_to_delete)

                self.logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ID {file_id} çš„ {len(ids_to_delete)} æ¡ç¬”è®°è®°å½•")
                return True
            else:
                self.logger.debug(f"æ–‡ä»¶ID {file_id} æ²¡æœ‰å…³è”çš„ç¬”è®°æ•°æ®")
                return True

        except Exception as e:
            self.logger.error(f"æ ¹æ®file_idåˆ é™¤æ–‡ä»¶æ•°æ®å¤±è´¥: {file_id}, é”™è¯¯: {e}")
            return False

    async def async_remove_file_data(self, file_path: str) -> bool:
        """
        åˆ é™¤ä¸æŒ‡å®šæ–‡ä»¶ç›¸å…³çš„æ‰€æœ‰æ•°æ®ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥çš„ChromaDBæ“ä½œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„remove_file_data
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                self.remove_file_data,
                file_path
            )
            return result

        except Exception as e:
            self.logger.error(f"å¼‚æ­¥åˆ é™¤æ–‡ä»¶ç›¸å…³æ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False

    def close(self):
        """å…³é—­æœåŠ¡ï¼Œé‡Šæ”¾èµ„æº"""
        try:
            # å…³é—­çº¿ç¨‹æ± 
            if hasattr(self, '_thread_pool'):
                self._thread_pool.shutdown(wait=True)
                self.logger.debug("çº¿ç¨‹æ± å·²å…³é—­")

            # å…³é—­IDæœåŠ¡
            if hasattr(self, 'id_service'):
                self.id_service.close()

            self.logger.debug("ç¬”è®°æœåŠ¡å·²å…³é—­")
        except Exception as e:
            self.logger.error(f"å…³é—­ç¬”è®°æœåŠ¡å¤±è´¥: {e}")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    @classmethod
    def from_plugin_context(cls, plugin_context):
        """
        ä»PluginContextåˆ›å»ºNoteServiceå®ä¾‹

        Args:
            plugin_context: PluginContextæ’ä»¶ä¸Šä¸‹æ–‡

        Returns:
            NoteServiceå®ä¾‹
        """
        return cls(plugin_context=plugin_context)

    def get_status(self):
        """
        è·å–æœåŠ¡çŠ¶æ€

        Returns:
            åŒ…å«çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
        """
        return {
            "ready": self.collections_initialized,
            "has_vector_store": self.vector_store is not None,
            "has_plugin_context": self.plugin_context is not None,
            "provider_id": self.id_service.provider_id if hasattr(self, 'id_service') else None,
            "batch_queue_size": len(self._embedding_queue) if hasattr(self, '_embedding_queue') else 0
        }

    async def _process_batch_embedding(self):
        """
        å¤„ç†æ‰¹é‡å‘é‡åŒ–é˜Ÿåˆ—
        
        Returns:
            (embedding_texts, documents, metadatas) å·²å¤„ç†çš„æ‰¹é‡æ•°æ®
        """
        if not self._embedding_queue:
            return [], [], []
            
        # å–å‡ºé˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ•°æ®
        batch_data = self._embedding_queue.copy()
        self._embedding_queue.clear()
        
        if not batch_data:
            return [], [], []
            
        # æå–å„ä¸ªå­—æ®µ
        embedding_texts = [item['embedding_text'] for item in batch_data]
        documents = [item['document'] for item in batch_data]
        metadatas = [item['metadata'] for item in batch_data]
        
        # self.logger.debug(f"æ‰¹é‡å‘é‡åŒ–: {len(embedding_texts)} ä¸ªæ–‡æ¡£å—")  # æ³¨é‡Šæ‰
        
        return embedding_texts, documents, metadatas

    async def _add_to_embedding_queue(self, embedding_text: str, document: str, metadata: dict) -> None:
        """
        å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡åŒ–é˜Ÿåˆ—
        
        Args:
            embedding_text: å¾…å‘é‡åŒ–çš„æ–‡æœ¬
            document: åŸå§‹æ–‡æ¡£å†…å®¹
            metadata: å…ƒæ•°æ®
        """
        async with self._batch_lock:
            self._embedding_queue.append({
                'embedding_text': embedding_text,
                'document': document,
                'metadata': metadata
            })
            
            # å¦‚æœé˜Ÿåˆ—è¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œè§¦å‘å¤„ç†
            if len(self._embedding_queue) >= self._batch_size:
                await self._process_batch_embedding()

    async def _flush_embedding_queue(self):
        """
        å¼ºåˆ¶å¤„ç†é˜Ÿåˆ—ä¸­å‰©ä½™çš„æ–‡æ¡£ï¼ˆç¡®ä¿æ‰€æœ‰æ–‡æ¡£éƒ½è¢«å¤„ç†ï¼‰
        """
        async with self._batch_lock:
            if self._embedding_queue:
                # self.logger.debug(f"å¼ºåˆ¶å¤„ç†å‰©ä½™ {len(self._embedding_queue)} ä¸ªæ–‡æ¡£å—")  # æ³¨é‡Šæ‰
                await self._process_batch_embedding()

    async def _store_notes_batch_optimized(self, notes: List[NoteData]) -> dict:
        """
        ä¼˜åŒ–ç‰ˆæœ¬çš„æ‰¹é‡å­˜å‚¨ç¬”è®°ï¼Œä½¿ç”¨é˜Ÿåˆ—æœºåˆ¶ç´¯ç§¯æ–‡æ¡£
        
        Args:
            notes: ç¬”è®°æ•°æ®åˆ—è¡¨
            
        Returns:
            è®¡æ—¶ä¿¡æ¯å­—å…¸
        """
        import time
        timings = {}
        
        if not notes:
            return timings
            
        t_start = time.time()
        
        # å‡†å¤‡ä¸»é›†åˆæ•°æ®
        main_collection_data = []
        sub_collection_data = []
        
        for note in notes:
            # ä¸»é›†åˆæ•°æ®
            import json
            main_collection_data.append({
                'id': note.id,
                'embedding_text': note.content,
                'document': note.content,
                'metadata': {
                    'file_id': note.file_id,
                    'tag_ids': json.dumps(note.tag_ids)  # ChromaDBä¸æ”¯æŒlistï¼Œéœ€è½¬JSONå­—ç¬¦ä¸²
                }
            })
            
            # å‰¯é›†åˆæ•°æ®ï¼ˆæ ‡ç­¾æ–‡æœ¬ï¼‰
            if note.tag_ids:
                tag_names = self.id_service.ids_to_tags(note.tag_ids)
                tags_text = ' '.join(tag_names)
                sub_collection_data.append({
                    'id': note.id,
                    'embedding_text': tags_text,
                    'document': tags_text,
                    'metadata': {'file_id': note.file_id}
                })
        
        # å°†æ•°æ®æ·»åŠ åˆ°æ‰¹é‡é˜Ÿåˆ—
        main_texts = []
        main_documents = []
        main_metadatas = []
        
        for data in main_collection_data:
            await self._add_to_embedding_queue(
                data['embedding_text'], 
                data['document'], 
                data['metadata']
            )
            main_texts.append(data['embedding_text'])
            main_documents.append(data['document'])
            main_metadatas.append(data['metadata'])
        
        # å¤„ç†é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ•°æ®
        await self._flush_embedding_queue()
        
        # æ‰¹é‡æ’å…¥åˆ°ä¸»é›†åˆ
        if main_texts:
            t_main = time.time()
            ids = [note.id for note in notes]
            upsert_timings = self.vector_store.upsert_documents(
                collection=self.main_collection,
                ids=ids,
                embedding_texts=main_texts,
                documents=main_documents,
                metadatas=main_metadatas,
                _return_timings=True
            )
            timings['store_main'] = (time.time() - t_main) * 1000
            if upsert_timings:
                timings['main_embed'] = upsert_timings.get('embed', 0)
                timings['main_db'] = upsert_timings.get('db_upsert', 0)
        
        # å¤„ç†å‰¯é›†åˆ
        if sub_collection_data:
            t_sub = time.time()
            sub_ids = [data['id'] for data in sub_collection_data]
            sub_texts = [data['embedding_text'] for data in sub_collection_data]
            sub_documents = [data['document'] for data in sub_collection_data]
            sub_metadatas = [data['metadata'] for data in sub_collection_data]
            
            sub_upsert_timings = self.vector_store.upsert_documents(
                collection=self.sub_collection,
                ids=sub_ids,
                embedding_texts=sub_texts,
                documents=sub_documents,
                metadatas=sub_metadatas,
                _return_timings=True
            )
            timings['store_sub'] = (time.time() - t_sub) * 1000
            if sub_upsert_timings:
                timings['sub_embed'] = sub_upsert_timings.get('embed', 0)
                timings['sub_db'] = sub_upsert_timings.get('db_upsert', 0)
        
        timings['total'] = (time.time() - t_start) * 1000
        return timings
